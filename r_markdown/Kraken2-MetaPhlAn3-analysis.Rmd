---
title: "Kraken2 and MetaPhlAn3 confidence and database testing - analysis of results"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
---

```{R, results='hide', fig.keep='all', message=FALSE}
library(reticulate)
#library(kableExtra)
library(knitr)
library(phyloseq)
library(microbiome)
```

```{python, results='hide', fig.keep='all', message=FALSE}
import os
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import numpy as np
import matplotlib as mpl
import math
from matplotlib.patches import Patch
from matplotlib.lines import Line2D
from numpy.linalg import norm
import matplotlib.cm as cm
import scipy.stats as stats
import matplotlib.colors as colors
from matplotlib.offsetbox import AnchoredText

direc = '/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/'
direc_db = direc+'database_classifications/'
direc_save = direc+'analysis/'
temp = direc+'temporary/'
samples = list(pd.read_csv(direc_db+'truth_rename_reads.csv', index_col=0, header=0).columns)

dbs = ['kraken2_minikraken', 'kraken2_standard_0521', 'kraken2_chocophlan', 'kraken2_refseqV208_nt', 'kraken2_GTDBr202RefSeqV205', 'kraken2_refseqV205']
size_limited_dbs = ['kraken2_refseqV205_100GB', 'kraken2_refseqV205_500GB']
other_dbs = ['kraken2_refseqV205_minimizers', 'MetaPhlAn']
confidence = ['0.00', '0.05', '0.10', '0.15', '0.20', '0.25', '0.30', '0.35', '0.40', '0.45', '0.50', '0.55', '0.60', '0.65', '0.70', '0.75', '0.80', '0.85', '0.90', '0.95', '1.00']

db_sizes = {'kraken2_chocophlan':r'$\bf{73 GB}$'+'\n13,475 taxa\n132,661 genomes', 'kraken2_minikraken':r'$\bf{8 GB}$'+'\n5,758 taxa\n ', 'kraken2_refseqV205':r'$\bf{1,189 GB/466 GB/94 GB}$'+'\n108,257 taxa\n227,889 genomes', 'kraken2_refseqV208_nt':r'$\bf{308 GB}$'+'\n113,002 taxa\n ', 'kraken2_GTDBr202RefSeqV205':r'$\bf{1,148 GB}$'+'\n59,472 taxa\n72,244 genomes'}
rename_db = {'kraken2_chocophlan':'ChocoPhlAn 3 equivalent', 'kraken2_minikraken':'MiniKraken2 V2', 'kraken2_refseqV205_100GB':'NCBI RefSeq Complete V205 100GB', 'kraken2_refseqV205_500GB':'NCBI RefSeq Complete V205 500GB', 'kraken2_refseqV205':'NCBI RefSeq Complete V205', 'kraken2_refseqV208_nt':'NCBI RefSeq V208 nt', 'kraken2_refseqV93':'NCBI RefSeq Complete V93', 'kraken2_GTDBr202RefSeqV205':'GTDB r202 + NCBI RefSeq Complete V205', 'MetaPhlAn 3':'MetaPhlAn 3', 'MetaPhlAn':'MetaPhlAn 3', 'kraken2_standard_0521':'NCBI RefSeq Standard (05/2021)'}
colors_db = {'kraken2_chocophlan':'#1A5276', 'kraken2_minikraken':'#9B59B6', 'kraken2_refseqV205':'#CB4335', 'kraken2_refseqV205_500GB':'#E67E22', 'kraken2_refseqV205_100GB':'#F1C40F', 'kraken2_refseqV208_nt':'#2ECC71', 'kraken2_refseqV93':'#2ECC71', 'kraken2_GTDBr202RefSeqV205':'#3498DB', 'MetaPhlAn 3':'#512E5F', 'MetaPhlAn':'#512E5F'}

alpha_div = ["Simpson's diversity", "Shannon diversity", "Faith's phylogenetic diversity", "Chao1 richness", "McIntosh's evenness", "Pielou evenness", "Simpson's evenness"]
beta_div = ["L1 distance", "Robust Aitchisons distance", "Bray-Curtis dissimilarity relative abundance", "Weighted unifrac distance relative abundance", "Unweighted unifrac distance relative abundance", "Aitchisons distance", "Bray-Curtis dissimilarity raw", "Weighted unifrac distance raw", "Unweighted unifrac distance raw"]
prec_rec_f1 = ["Proportion classified", "Precision taxa", "Recall taxa", "F1 score taxa", "Precision reads", "Recall reads", "F1 score reads", "Mean F1 score"]
all_metrics_together = ["Precision taxa", "Recall taxa", "F1 score taxa", "Proportion classified", "Precision reads", "Recall reads", "F1 score reads", "Mean F1 score"]+alpha_div+beta_div[1:]+[beta_div[0]]

limits = {"Proportion classified":[0.8, 1.01], "Simpson's diversity":[-0.1, 0.05], "Shannon diversity":[-1, 0.5], "Faith's phylogenetic diversity":[-200, 12500], "Chao1 richness":[-200, 10000], "McIntosh's evenness":[-0.05, 0.1], "Pielou evenness":[-0.3, 0.1], "Simpson's evenness": [-0.2, 0.05], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 50], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

limits_metaphlan = {"Proportion classified":[-0.01, 1.01], "Simpson's diversity":[-0.05, 0.25], "Shannon diversity":[0, 7], "Faith's phylogenetic diversity":[0, 3000], "Chao1 richness":[500, 2000], "McIntosh's evenness":[0.2, 1], "Pielou evenness":[-0.1, 0.6], "Simpson's evenness": [0.6, 1.05], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 100], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

limits_all = {"Proportion classified":[-0.01, 1.01], "Simpson's diversity":[-0.05, 0.3], "Shannon diversity":[0, 10], "Faith's phylogenetic diversity":[0, 55000], "Chao1 richness":[0, 45000], "McIntosh's evenness":[0.2, 1], "Pielou evenness":[-0.1, 0.6], "Simpson's evenness": [0.6, 1.05], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 100], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

locations = {"Simpson's diversity":'lower left', "Shannon diversity":'lower left', "Faith's phylogenetic diversity":'upper right', "Chao1 richness":'upper right', "McIntosh's evenness":'upper left', "Pielou evenness":'lower right', "Simpson's evenness":'lower right', "L1 distance":'upper left', "Robust Aitchisons distance":'upper left', "Bray-Curtis dissimilarity relative abundance":'upper left', "Weighted unifrac distance relative abundance":'upper left', "Unweighted unifrac distance relative abundance":'upper left', "Aitchisons distance":'upper right', "Bray-Curtis dissimilarity raw":'upper left', "Weighted unifrac distance raw":'upper left', "Unweighted unifrac distance raw":'upper right', "Proportion classified":'lower left', "Precision taxa":'upper left', "Recall taxa":'lower left', "F1 score taxa":'lower right', "Precision reads":'lower center', "Recall reads":'lower center', "F1 score reads":'lower center', "Mean F1 score":'lower center'}

y_labels = {"Proportion classified":'Proportion', "Precision taxa":'Proportion', "Recall taxa":'Proportion', "F1 score taxa":'F1 score', "Precision reads":'Proportion', "Recall reads":'Proportion', "F1 score reads":'F1 score', "Mean F1 score":'F1 score', "Simpson's diversity":'Difference between classification\nand known composition', "Shannon diversity":'Difference between classification\nand known composition', "Faith's phylogenetic diversity":'Difference between classification\nand known composition', "Chao1 richness":'Difference between classification\nand known composition', "McIntosh's evenness":'Difference between classification\nand known composition', "Pielou evenness":'Difference between classification\nand known composition', "Simpson's evenness":'Difference between classification\nand known composition', "L1 distance":'Distance between classification\nand known composition', "Robust Aitchisons distance":'Distance between classification\nand known composition', "Bray-Curtis dissimilarity relative abundance":'Dissimilarity between classification\nand known composition', "Weighted unifrac distance relative abundance":'Distance between classification\nand known composition', "Unweighted unifrac distance relative abundance":'Distance between classification\nand known composition', "Aitchisons distance":'Distance between classification\nand known composition', "Bray-Curtis dissimilarity raw":'Dissimilarity between classification\nand known composition', "Weighted unifrac distance raw":'Distance between classification\nand known composition', "Unweighted unifrac distance raw":'Distance between classification\nand known composition'}

rename_settings = {'default':'Default relative abundance\nx number of reads', 'estimated_reads':'Default estimated reads\n(tavg_g/very-sensitive)', 'estimated_reads':'very-sensitive\n(Default estimated reads)', 'very_sensitive_local':'very-sensitive-local', 'sensitive':'sensitive', 'sensitive_local':'sensitive-local', 'avg_g':'avg_g: clade global avergae', 'avg_l':'avg_l: average of length-\nnormalized marker counts', 'estimated_reads':'tavg_g: truncated clade global average\n(Default estimated reads)', 'tavg_l':'tavg_l: truncated average of length-\nnormalized marker counts', 'wavg_g':'wavg_g: winsorized clade global average', 'wavg_l':'wavg_l: winsorized average of length-\nnormalized marker counts', 'med':'med: median of length-normalized\nmarker counts', 'estimated_reads':'Estimated reads (tavg_g)', 'humann_bowtie2':'HUMAnN 3 Bowtie2\nnucleotide alignment', 'humann_diamond':'HUMAnN 3 Diamond\ntranslated alignment'}

saving_figures = True
```

# 1. Number of citations per tool

```{python, results='hide', fig.keep='all'}
tools = ['Kraken2', 'Kraken', 'CLARK', 'MetaPalette', 'DUDes', 'FOCUS', 'MetaPhlAn 3', 'MetaPhlAn2', 'MetaPhlAn', 'MetaPhyler', 'mOTU', 'Quikr', 'Ark', 'Sek', 'Taxy-Pro', 'TIPP', 'Centrifuge', 'CLARK-S', 'KrakenUniq', 'MegaBLAST', 'metaOthello','PathSeq', 'Prophyle', 'taxMaps']
publication_years = [2019, 2014, 2015, 2016, 2016, 2014, 2021, 2015, 2012, 2011, 2013, 2013, 2015, 2014, 2013, 2014, 2016, 2016, 2018, 2008, 2018, 2011, 2017, 2018]
citations = [807, 2737, 447, 35, 36, 92, 54, 1258, 1394, 222, 392, 54, 9, 17, 39, 70, 637, 76, 125, 987, 33, 269, 1, 20]

plt.figure(figsize=(8,5))
ax1 = plt.subplot(111)
ax2 = ax1.twinx()
order = [a for a in range(len(tools))]
new_order = [x for _, x in sorted(zip(publication_years, order))]

fontcolors = []
xplot = 0
for t in new_order:
  print(tools[t], publication_years[t], t)
  if 'Kraken' not in tools[t] and 'MetaPhlAn' not in tools[t]: fc='k'
  else: fc='#A93226'
  color, color2 = '#1F618D', '#85C1E9'
  ax1.bar(xplot-0.225, citations[t], color=color, width=0.4)
  years_since_publication = (2021-float(publication_years[t]))
  if years_since_publication == 0: years_since_publication = 1
  ax2.bar(xplot+0.225, float(citations[t])/years_since_publication, color=color2, width=0.4)
  ax1.text(xplot, -80, tools[t]+' ('+str(publication_years[t])+')', color=fc, rotation=90, ha='center', va='top')
  xplot += 1

ax1.set_xlim([-0.75, max(order)+0.75])
ax2.set_xlim([-0.75, max(order)+0.75])
# plt.sca(ax1)
# plt.xticks(xtick, tools_sorted, color=fontcolors, rotation=90)
plt.xticks([a for a in range(len(tools))], ['' for a in range(len(tools))])
ax1.set_ylabel('Total number of citations', color=color)
ax2.set_ylabel('Number of citations per year', color=color2)

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/citations_tools.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
plt.close()
```

# 2. Number of reads and taxa covered by each database {.tabset}

## Horizontal plot

```{python, results='hide', fig.keep='all'}
db_sizes = {'kraken2_chocophlan':r'$\bf{73 GB}$'+'\n13,475 taxa\n132,661 genomes', 'kraken2_minikraken':r'$\bf{8 GB}$'+'\n5,758 taxa\n ', 'kraken2_refseqV205':r'$\bf{1,189 GB/466 GB/94 GB}$'+'\n108,257 taxa\n227,889 genomes', 'kraken2_refseqV208_nt':r'$\bf{308 GB}$'+'\n113,002 taxa\n ', 'kraken2_GTDBr202RefSeqV205':r'$\bf{1,148 GB}$'+'\n59,472 taxa\n72,244 genomes'}
this_dbs = ['kraken2_minikraken', 'kraken2_chocophlan', 'kraken2_refseqV208_nt', 'kraken2_GTDBr202RefSeqV205', 'kraken2_refseqV205']
db_names = ['MiniKraken2 V2', 'ChocoPhlAn 3', 'NCBI RefSeq V208 nt', 'GTDB r202 (bacteria/archaea)\n+ NCBI RefSeq V205 (other domains)', 'NCBI RefSeq Complete V205']
truth_taxa = pd.read_csv(direc+'databases/truth_proportion_taxa_covered.csv', index_col=0, header=0)
truth_reads = pd.read_csv(direc+'databases/truth_proportion_reads_covered.csv', index_col=0, header=0)
fig = plt.figure(figsize=(12,4))
#fig.suptitle('Proportion of truth samples covered by database', fontsize=14, fontweight='bold')
ax1 = plt.subplot(121)
ax2 = plt.subplot(122)

for d in range(len(this_dbs)):
  print(this_dbs[d])
  covered = list(truth_taxa.loc[:, this_dbs[d]].values)
  print('Taxa', np.median(covered), min(covered), max(covered))
  ax1.scatter(covered, np.random.normal(d, scale=0.075, size=len(covered)), color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax1.boxplot(covered, positions=[d], showfliers=False, widths=0.5, vert=False)
  plt.sca(ax1)
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

  covered = list(truth_reads.loc[:, this_dbs[d]].values)
  print('Reads', np.median(covered), min(covered), max(covered))
  ax2.scatter(covered, np.random.normal(d, scale=0.075, size=len(covered)), color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax2.boxplot(covered, positions=[d], showfliers=False, widths=0.5, vert=False)
  plt.sca(ax2)
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

plt.sca(ax1)
plt.yticks([d for d in range(len(this_dbs))], db_names, fontweight='bold')
#plt.ylabel('', fontweight='bold')
plt.xlabel('Proportion')
plt.title('Proportion of taxa\ncovered by database', fontweight='bold')

plt.sca(ax2)
#plt.yticks([d for d in range(len(this_dbs))], [db_sizes[this_dbs[d]] for d in range(len(this_dbs))])
plt.yticks([d for d in range(len(this_dbs))], ['' for d in range(len(this_dbs))])
plt.xlabel('Proportion')
[plt.text(1.075, d, db_sizes[this_dbs[d]], ha='left', va='center') for d in range(len(this_dbs))]
#ax2.yaxis.tick_right()
plt.title('Proportion of reads\ncovered by database', fontweight='bold')

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/Databases_proportion_covered.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
plt.close()
```

## Alternative plots {.tabset}

## Poster plot

```{python, results='hide', fig.keep='all'}
db_sizes = {'kraken2_chocophlan':r'$\bf{73 GB}$'+'\n13,475 taxa\n132,661 genomes', 'kraken2_minikraken':r'$\bf{8 GB}$'+'\n5,758 taxa\n ', 'kraken2_refseqV205':r'$\bf{1,189 GB/466 GB/94 GB}$'+'\n108,257 taxa\n227,889 genomes', 'kraken2_refseqV208_nt':r'$\bf{308 GB}$'+'\n113,002 taxa\n ', 'kraken2_GTDBr202RefSeqV205':r'$\bf{1,148 GB}$'+'\n59,472 taxa\n72,244 genomes'}
this_dbs = ['kraken2_minikraken', 'kraken2_chocophlan', 'kraken2_refseqV208_nt', 'kraken2_GTDBr202RefSeqV205', 'kraken2_refseqV205']
db_names = ['Kraken2\nMiniKraken2 V2', 'ChocoPhlAn 3\n(MetaPhlAn 3 &\nKraken2 equivalent)', 'Kraken2\nRefSeq V208 nt', 'Kraken2\nGTDB r202\n+ RefSeq V205', 'Kraken2\nRefSeq V205']
truth_taxa = pd.read_csv(direc+'databases/truth_proportion_taxa_covered.csv', index_col=0, header=0)
truth_reads = pd.read_csv(direc+'databases/truth_proportion_reads_covered.csv', index_col=0, header=0)
fig = plt.figure(figsize=(15,5))
#fig.suptitle('Proportion of truth samples covered by database', fontsize=14, fontweight='bold')
ax1 = plt.subplot(121)
ax2 = plt.subplot(122)

for d in range(len(this_dbs)):
  print(this_dbs[d])
  covered = list(truth_taxa.loc[:, this_dbs[d]].values)
  print('Taxa', np.median(covered), min(covered), max(covered))
  ax1.scatter(covered, np.random.normal(d, scale=0.075, size=len(covered)), color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax1.boxplot(covered, positions=[d], showfliers=False, widths=0.5, vert=False)
  plt.sca(ax1)
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

  covered = list(truth_reads.loc[:, this_dbs[d]].values)
  print('Reads', np.median(covered), min(covered), max(covered))
  ax2.scatter(covered, np.random.normal(d, scale=0.075, size=len(covered)), color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax2.boxplot(covered, positions=[d], showfliers=False, widths=0.5, vert=False)
  plt.sca(ax2)
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

plt.sca(ax1)
plt.yticks([d for d in range(len(this_dbs))], db_names, fontweight='bold')
#plt.ylabel('', fontweight='bold')
plt.xlabel('Proportion')
plt.title('Proportion of taxa', fontweight='bold')

plt.sca(ax2)
#plt.yticks([d for d in range(len(this_dbs))], [db_sizes[this_dbs[d]] for d in range(len(this_dbs))])
plt.yticks([d for d in range(len(this_dbs))], ['' for d in range(len(this_dbs))])
plt.xlabel('Proportion')
[plt.text(1.075, d, db_sizes[this_dbs[d]], ha='left', va='center') for d in range(len(this_dbs))]
#ax2.yaxis.tick_right()
plt.title('Proportion of reads', fontweight='bold')

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/Databases_proportion_covered_poster.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
plt.close()
```

### Vertical

```{python, results='hide', fig.keep='all'}
this_dbs = ['kraken2_chocophlan', 'kraken2_minikraken', 'kraken2_refseqV205', 'kraken2_refseqV208_nt', 'kraken2_GTDBr202RefSeqV205']
db_names = ['ChocoPhlAn 3\n(MetaPhlAn 3 &\nKraken2 equivalent)', 'Kraken2\nMiniKraken2 V2', 'Kraken2\nRefSeq V205', 'Kraken2\nRefSeq V208 nt', 'Kraken2\nGTDB r202\n+ RefSeq V205']
truth_taxa = pd.read_csv(direc+'databases/truth_proportion_taxa_covered.csv', index_col=0, header=0)
truth_reads = pd.read_csv(direc+'databases/truth_proportion_reads_covered.csv', index_col=0, header=0)
plt.figure(figsize=(10,10))
ax1 = plt.subplot(211)
ax2 = plt.subplot(212)

for d in range(len(this_dbs)):
  covered = list(truth_taxa.loc[:, this_dbs[d]].values)
  ax1.scatter(np.random.normal(d, scale=0.075, size=len(covered)), covered, color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax1.boxplot(covered, positions=[d], showfliers=False, widths=0.5)
  plt.sca(ax1)
  print('Taxa', this_dbs[d], 'Mean=', np.mean(covered), 'Median=', np.median(covered), 'Minimum=', min(covered), 'Maximum=', max(covered), 'Upper quartile=', np.percentile(covered, 75), 'Lower quartile=', np.percentile(covered, 75))
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
  
  covered = list(truth_reads.loc[:, this_dbs[d]].values)
  ax2.scatter(np.random.normal(d, scale=0.075, size=len(covered)), covered, color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax2.boxplot(covered, positions=[d], showfliers=False, widths=0.5)
  plt.sca(ax2)
  print('Reads', this_dbs[d], 'Mean=', np.mean(covered), 'Median=', np.median(covered), 'Minimum=', min(covered), 'Maximum=', max(covered), 'Upper quartile=', np.percentile(covered, 75), 'Lower quartile=', np.percentile(covered, 75))
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

plt.sca(ax1)
plt.xticks([d for d in range(len(this_dbs))], ['' for d in range(len(this_dbs))])
plt.ylabel('Proportion of taxa', fontweight='bold')
plt.title('Proportion of truth samples covered by database', fontweight='bold')

plt.sca(ax2)
plt.xticks([d for d in range(len(this_dbs))], db_names)
plt.ylabel('Proportion of reads', fontweight='bold')
plt.show()
plt.close()
```

### Horizontal

```{python, results='hide', fig.keep='all'}
# this_dbs = dbs[:2]+dbs[4:]
this_dbs = ['kraken2_chocophlan', 'kraken2_minikraken', 'kraken2_refseqV205', 'kraken2_refseqV208_nt', 'kraken2_GTDBr202RefSeqV205']
this_dbs.reverse()
db_names = ['ChocoPhlAn 3\n(MetaPhlAn 3 &\nKraken2 equivalent)', 'Kraken2\nMiniKraken2 V2', 'Kraken2\nRefSeq V205', 'Kraken2\nRefSeq V208 nt', 'Kraken2\nGTDB r202\n+ RefSeq V205']
db_names.reverse()
truth_taxa = pd.read_csv(direc+'databases/truth_proportion_taxa_covered.csv', index_col=0, header=0)
truth_reads = pd.read_csv(direc+'databases/truth_proportion_reads_covered.csv', index_col=0, header=0)
fig = plt.figure(figsize=(10,5))
fig.suptitle('Proportion of truth samples covered by database', fontsize=14, fontweight='bold')
ax1 = plt.subplot(121)
ax2 = plt.subplot(122)

for d in range(len(this_dbs)):
  covered = list(truth_taxa.loc[:, this_dbs[d]].values)
  ax1.scatter(covered, np.random.normal(d, scale=0.075, size=len(covered)), color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax1.boxplot(covered, positions=[d], showfliers=False, widths=0.5, vert=False)
  plt.sca(ax1)
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

  covered = list(truth_reads.loc[:, this_dbs[d]].values)
  ax2.scatter(covered, np.random.normal(d, scale=0.075, size=len(covered)), color=colors_db[this_dbs[d]], s=10, alpha=0.2)
  box = ax2.boxplot(covered, positions=[d], showfliers=False, widths=0.5, vert=False)
  plt.sca(ax2)
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

plt.sca(ax1)
plt.yticks([d for d in range(len(this_dbs))], db_names)
#plt.ylabel('', fontweight='bold')
plt.xlabel('Proportion')
plt.title('Proportion of taxa', fontweight='bold')

plt.sca(ax2)
plt.yticks([d for d in range(len(this_dbs))], ['' for d in range(len(this_dbs))])
#plt.ylabel('Proportion of reads', fontweight='bold')
plt.xlabel('Proportion')
plt.title('Proportion of reads', fontweight='bold')
plt.show()
plt.close()
```

# 3. Confidence threshold comparison with NCBI RefSeq V205 only {.tabset}

Perhaps for each of these I should initially show all samples and then focus on the different Parks et al. sets as these represent more realistic samples? And they have 10 of each.
Look at F1 scores for each separately of these. 
Less focus on whether they can tell two strains apart, but more on low/high species diversity and ANI.

## All samples {.tabset}

### All metrics together

```{python, results='hide', fig.keep='all'}
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,35))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', header=0, index_col=0)

c = 1
axes  = []
for a in range(7):
  for b in range(4):
    if a == 3 and b == 3: continue
    if a == 6 and b > 0: continue
    ax = plt.subplot2grid((32,4), (c,b), rowspan=4)
    axes.append(ax)
  c += 4
  if a in [1,3]: c += 1
  
axes[0].text(0, 1.2, 'A     Proportion of reads classified, precision, recall and F1 score', fontweight='bold', ha='left', va='center', fontsize=16, transform=axes[0].transAxes)
axes[8].text(0, 1.2, 'B     Alpha diversity, richness and evenness', fontweight='bold', ha='left', va='center', fontsize=16, transform=axes[8].transAxes)
axes[15].text(0, 1.2, 'C     Beta diversity', fontweight='bold', ha='left', va='center', fontsize=16, transform=axes[15].transAxes)

for m in range(len(all_metrics_together)):
  metric = all_metrics_together[m]
  ax = axes[m]
  plt.sca(ax)
  ax.set_title(metric.replace('Proportion classified', 'Proportion of reads classified').replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw').replace('phylogenetic', 'taxonomic').replace('unifrac distance', 'unifrac taxonomic distance'), fontweight='bold')

  all_samples = [[] for conf in confidence]
  for sample in samples:
    this_sample, this_conf = [], []
    for c in range(len(confidence)):
      try:
        this_val = this_db.loc[sample+'-'+db+'-'+confidence[c], metric]
      except:
        this_val = 0
      if metric in alpha_div or metric in beta_div:
        if this_val == 0:
          this_val = max(this_db.loc[:, metric].values)
      if metric in alpha_div:
        this_val = this_val-truth_calcs.loc[sample, metric]
      this_sample.append(this_val)
      all_samples[c].append(this_val)
      this_conf.append(float(confidence[c]))
    ax.plot(this_conf, this_sample, 'k-', alpha=0.05)
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  if m in [4,5,6,7,11,12,13,14,20,21,22,23]:
    plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
    plt.xlabel('Confidence threshold')
  else: plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence])
  plt.xlim([-0.025, 1.025])

  if metric in limits_all:
    plt.ylim(limits[metric])
  elif metric in prec_rec_f1:
    plt.ylim([-0.05, 1.05])
  
  if metric not in alpha_div and metric not in beta_div:
    plt.ylabel(y_labels[metric], fontsize=8)
  elif m in [8,12,15,19,23]:
    plt.ylabel(y_labels[metric], fontsize=8)

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in overall])
    max_index = [abs(val) for val in overall].index(max_value)
    max_value = overall[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+'\nat confidence\nthreshold = '+confidence[max_index]
  else:
    min_value = min([abs(val) for val in overall])
    min_index = [abs(val) for val in overall].index(min_value)
    min_value = overall[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+'\nat confidence\nthreshold = '+confidence[min_index]

  anchored_text = AnchoredText(string, loc=locations[metric])
  ax.add_artist(anchored_text)

#plt.tight_layout()
plt.subplots_adjust(hspace=3)
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_all_metrics_lines.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### Precision, recall, F1 score

```{python, results='hide', fig.keep='all'}
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

for a in range(8):
  if a == 0:
    ax = plt.subplot2grid((4,5),(1,0), rowspan=2)
  elif a == 7:
    ax = plt.subplot2grid((4,5),(1,4), rowspan=2)
  elif a > 3:
    ax = plt.subplot2grid((4,5), (2,a-3), rowspan=2)
  else:
    ax = plt.subplot2grid((4,5), (0,a), rowspan=2)
  ax.set_title(prec_rec_f1[a].replace('relative abundance', '\nrelative abundance').replace('raw', '\n raw'), fontweight='bold')
  all_samples = [[] for conf in confidence]
  for sample in samples:
    this_sample, this_conf = [], []
    for c in range(len(confidence)):
      try:
        this_val = this_db.loc[sample+'-'+db+'-'+confidence[c], prec_rec_f1[a]]
      except: 
        this_val = 0
      this_sample.append(this_val)
      all_samples[c].append(this_val)
      this_conf.append(float(confidence[c]))
    ax.plot(this_conf, this_sample, 'k-', alpha=0.05)
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  if a == 0 or a > 3: 
    plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
    plt.xlabel('Confidence threshold')
  else: plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence])
  plt.xlim([-0.025, 1.025])
  
  max_value = max(overall)
  max_index = overall.index(max_value)
  if a == 0 or a > 3:
    plt.text(0.25, 0.2, 'Maximum = '+str(round(max_value, 3))+'\n at confidence\nthreshold = '+confidence[max_index], ha='center', va='top')
  else:
    plt.text(0.75, 0.2, 'Maximum = '+str(round(max_value, 3))+'\n at confidence\nthreshold = '+confidence[max_index], ha='center', va='top')

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_prec_recall_f1_lines.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### Alpha diversity

Zeroes removed:
```{python, results='hide', fig.keep='all'}
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
db = 'kraken2_refseqV205'
refseq_calcs = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)

plt.figure(figsize=(15,8))
lims = [[-0.25, 0.25], [-2.5, 1.25], [-2000, 20000], [-2000, 20000], [-0.3, 0.3], [-0.5, 0.25], [-0.3, 0.15]]
locs = [[0.25, 0.2], [0.25, 0.2], [0.75, 0.9], [0.75, 0.9], [0.75, 0.2], [0.75, 0.2], [0.75, 0.2]]

for m in range(len(alpha_div)):
  metric = alpha_div[m]
  ax = plt.subplot(2,4,m+1)
  plt.sca(ax)
  all_confidence = [[] for conf in confidence]
  for sample in samples:
    this_difference, this_confidence = [], []
    for c in range(len(confidence)):
      conf = confidence[c]
      try:
        v1 = refseq_calcs.loc[sample+'-'+db+'-'+conf, metric]
        v2 = truth_calcs.loc[sample, metric]
        this_difference.append(v1-v2)
        all_confidence[c].append(v1-v2)
        this_confidence.append(float(conf))
      except:
        do_nothing = True
    ax.plot(this_confidence, this_difference, 'k-', alpha=0.05)
    
  overall, upper, lower = [], [], []
  for b in range(len(all_confidence)):
    overall.append(np.median(all_confidence[b]))
    upper.append(np.percentile(all_confidence[b], 75))
    lower.append(np.percentile(all_confidence[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  
  min_value = min([abs(val) for val in overall])
  min_index = [abs(val) for val in overall].index(min_value)
  min_value = overall[min_index]
  plt.text(locs[m][0], locs[m][1], 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index], ha='center', va='top', transform=ax.transAxes)
  
  if m < 3: plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence], rotation=90)
  else: plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
  if m == 0 or m == 4: plt.ylabel('Difference between classified\ndiversity and actual diversity')
  plt.title(metric, fontweight='bold')
  plt.xlim([-0.025, 1.025])
  plt.plot([-0.025, 1.025], [0, 0], 'k--')
  plt.ylim(limits[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_alpha_diversity_lines_zero_removed.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

Zeroes replaced with maximum value:
```{python, results='hide', fig.keep='all'}
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
db = 'kraken2_refseqV205'
refseq_calcs = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)

plt.figure(figsize=(15,8))
lims = [[-0.25, 0.25], [-2.5, 1.25], [-2000, 20000], [-2000, 20000], [-0.3, 0.3], [-0.5, 0.25], [-0.3, 0.15]]
locs = [[0.25, 0.2], [0.25, 0.2], [0.75, 0.9], [0.75, 0.9], [0.75, 0.2], [0.75, 0.2], [0.75, 0.2]]

for m in range(len(alpha_div)):
  metric = alpha_div[m]
  ax = plt.subplot(2,4,m+1)
  plt.sca(ax)
  all_confidence = [[] for conf in confidence]
  for sample in samples:
    this_difference, this_confidence = [], []
    for c in range(len(confidence)):
      conf = confidence[c]
      try:
        v1 = refseq_calcs.loc[sample+'-'+db+'-'+conf, metric]
      except:
        v1 = max(refseq_calcs.loc[:, metric].values)
      v2 = truth_calcs.loc[sample, metric]
      this_difference.append(v1-v2)
      all_confidence[c].append(v1-v2)
      this_confidence.append(float(conf))
    ax.plot(this_confidence, this_difference, 'k-', alpha=0.05)
    
  overall, upper, lower = [], [], []
  for b in range(len(all_confidence)):
    overall.append(np.median(all_confidence[b]))
    upper.append(np.percentile(all_confidence[b], 75))
    lower.append(np.percentile(all_confidence[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  
  min_value = min([abs(val) for val in overall])
  min_index = [abs(val) for val in overall].index(min_value)
  min_value = overall[min_index]
  plt.text(locs[m][0], locs[m][1], 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index], ha='center', va='top', transform=ax.transAxes)
  
  if m < 3: plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence], rotation=90)
  else: plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
  if m == 0 or m == 4: plt.ylabel('Difference between classified\ndiversity and actual diversity')
  plt.title(metric, fontweight='bold')
  plt.xlim([-0.025, 1.025])
  plt.plot([-0.025, 1.025], [0, 0], 'k--')
  plt.ylim(limits[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_alpha_diversity_lines_zero_replaced_with_max.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### Beta diversity

Zeroes removed:
```{python, results='hide', fig.keep='all'}
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

locs = [[0.25, 0.9], [0.25, 0.9], [0.25, 0.9], [0.25, 0.9], [0.25, 0.9], [0.75, 0.9], [0.25, 0.9], [0.25, 0.9], [0.75, 0.9]]

for a in range(9):
  if a == 0:
    ax = plt.subplot2grid((4,5),(1,0), rowspan=2)
  elif a > 4:
    ax = plt.subplot2grid((4,5), (2,a-4), rowspan=2)
  else:
    ax = plt.subplot2grid((4,5), (0,a), rowspan=2)
  ax.set_title(beta_div[a].replace('relative abundance', '\nrelative abundance').replace('raw', '\n raw'), fontweight='bold')
  all_samples = [[] for conf in confidence]
  for sample in samples:
    this_sample, this_conf = [], []
    for c in range(len(confidence)):
      try:
        this_div = this_db.loc[sample+'-'+db+'-'+confidence[c], beta_div[a]]
        if this_div == 0: continue
        this_sample.append(this_div)
        all_samples[c].append(this_div)
        this_conf.append(float(confidence[c]))
      except: do_nothing = True
    ax.plot(this_conf, this_sample, 'k-', alpha=0.05)
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  
  min_value = min([abs(val) for val in overall])
  min_index = [abs(val) for val in overall].index(min_value)
  min_value = overall[min_index]
  plt.text(locs[a][0], locs[a][1], 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index], ha='center', va='top', transform=ax.transAxes)
  
  if a == 0 or a > 4: 
    plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
    plt.xlabel('Confidence threshold')
  else: plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence])
  plt.xlim([-0.025, 1.025])
  plt.ylim(limits[beta_div[a]])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_beta_diversity_lines_zero_removed.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

Zeroes replaced with maximum value:
```{python, results='hide', fig.keep='all' }
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

locs = [[0.25, 0.9], [0.25, 0.9], [0.25, 0.9], [0.25, 0.9], [0.25, 0.9], [0.75, 0.9], [0.25, 0.9], [0.25, 0.9], [0.75, 0.9]]

for a in range(9):
  if a == 0:
    ax = plt.subplot2grid((4,5),(1,0), rowspan=2)
  elif a > 4:
    ax = plt.subplot2grid((4,5), (2,a-4), rowspan=2)
  else:
    ax = plt.subplot2grid((4,5), (0,a), rowspan=2)
  ax.set_title(beta_div[a].replace('relative abundance', '\nrelative abundance').replace('raw', '\n raw'), fontweight='bold')
  all_samples = [[] for conf in confidence]
  for sample in samples:
    this_sample, this_conf = [], []
    for c in range(len(confidence)):
      try:
        this_div = this_db.loc[sample+'-'+db+'-'+confidence[c], beta_div[a]]
        if this_div == 0:
          this_div = max(this_db.loc[:, beta_div[a]].values)
      except: 
        this_div = max(this_db.loc[:, beta_div[a]].values)
      this_sample.append(this_div)
      all_samples[c].append(this_div)
      this_conf.append(float(confidence[c]))
    ax.plot(this_conf, this_sample, 'k-', alpha=0.05)
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  
  min_value = min([abs(val) for val in overall])
  min_index = [abs(val) for val in overall].index(min_value)
  min_value = overall[min_index]
  plt.text(locs[a][0], locs[a][1], 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index], ha='center', va='top', transform=ax.transAxes)
  
  if a == 0 or a > 4: 
    plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
    plt.xlabel('Confidence threshold')
  else: plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence])
  plt.xlim([-0.025, 1.025])
  plt.ylim(limits[beta_div[a]])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_beta_diversity_lines_zero_replaced_with_max.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### Reduced metrics

```{python, results='hide', fig.keep='all'}
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
limited_locations = {'Proportion classified':'lower left', 'Mean F1 score':'lower center', "Simpson's diversity":'lower left', 'L1 distance':'upper right'}

for a in range(len(limited_metrics)):
  ax = plt.subplot2grid((4,5),(1,a), rowspan=2)
  ax.set_title(limited_metrics[a].replace('Proportion classified', 'Proportion of reads classified'), fontweight='bold')
  plt.sca(ax)
  all_samples = [[] for conf in confidence]
  for sample in samples:
    this_sample, this_conf = [], []
    for c in range(len(confidence)):
      try:
        this_val = this_db.loc[sample+'-'+db+'-'+confidence[c], limited_metrics[a]]
      except: 
        if limited_metrics[a] in alpha_div or limited_metrics[a] in beta_div:
          this_val = max(this_db.loc[:, limited_metrics[a]].values)
        else:
          this_val = 0
      if limited_metrics[a] in alpha_div:
        this_val = this_val-truth_calcs.loc[sample, limited_metrics[a]]
      this_sample.append(this_val)
      all_samples[c].append(this_val)
      this_conf.append(float(confidence[c]))
    ax.plot(this_conf, this_sample, 'k-', alpha=0.05)
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
  plt.xlabel('Confidence threshold')
  plt.xlim([-0.025, 1.025])
  
  if limited_metrics[a] in prec_rec_f1:
    max_value = max([abs(val) for val in overall])
    max_index = [abs(val) for val in overall].index(max_value)
    max_value = overall[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+'\n at confidence\nthreshold = '+confidence[max_index]
  else:
    min_value = min([abs(val) for val in overall])
    min_index = [abs(val) for val in overall].index(min_value)
    min_value = overall[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index]

  anchored_text = AnchoredText(string, loc=limited_locations[limited_metrics[a]])
  ax.add_artist(anchored_text)
  if limited_metrics[a] in limits: plt.ylim(limits[limited_metrics[a]])
  else: plt.ylim([0.2, 0.8])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_reduced_metrics_lines.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### Correlations

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1+alpha_div+beta_div
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
plt.figure(figsize=(15,15))
ax1 = plt.subplot(111)
plt.sca(ax1)
plot_col_rev = list(plot_columns)
plot_col_rev.reverse()

spearman, pval = [], []
for a in range(len(plot_col_rev)):
  this_spearman, this_pval = [], []
  for b in range(len(plot_col_rev)):
    first, second = [], []
    for sample in samples:
      for conf in confidence:
        try:
          first.append(this_db.loc[sample+'-'+db+'-'+confidence[c], plot_col_rev[a]])
          second.append(this_db.loc[sample+'-'+db+'-'+confidence[c], plot_col_rev[b]])
        except:
          do_nothing = True
    corr = stats.spearmanr(first, second)
    this_spearman.append(float(corr[0]))
    this_pval.append(float(corr[1]))
  spearman.append(this_spearman)
  pval.append(this_pval)
  
spearman_df = pd.DataFrame(spearman, columns=plot_col_rev, index=plot_col_rev)

heatmap = plt.pcolor(spearman_df, cmap='bwr', vmin=-1, vmax=1, edgecolor='k')
cbar = plt.colorbar(heatmap)

plt.xticks([a+0.5 for a in range(len(plot_col_rev))], plot_col_rev, rotation=90)
plt.yticks([a+0.5 for a in range(len(plot_col_rev))], plot_col_rev)
ax1.xaxis.tick_top()

for a in range(len(plot_columns)):
  for b in range(len(plot_columns)):
    if pval[a][b] <= 0.01:
      plt.text(a+0.5, b+0.4, '*', ha='center', va='center')

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_correlations.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
    
```

## Different sample characteristics all confidence {.tabset}

```{python, results='hide', fig.keep='all' }
from matplotlib.offsetbox import AnchoredText

def get_plot_single_metric(ax, df, truth_df, these_samples, metric):
  all_samples = [[] for conf in confidence]
  for sample in these_samples:
    these_vals = []
    for c in range(len(confidence)):
      conf = confidence[c]
      try:
        val = df.loc[sample+'-'+db+'-'+conf, metric]
      except:
        if metric in prec_rec_f1:
          val = 0
        elif metric in alpha_div or metric in beta_div:
          val = max(df.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_df.loc[sample, metric]
      these_vals.append(val)
      all_samples[c].append(val)
    ax.plot([float(conf) for conf in confidence], these_vals, 'k-', alpha=0.3)
    
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
  if metric in alpha_div:
    ax.plot([float(conf) for conf in confidence], [0 for conf in confidence], 'k--')
  plt.sca(ax)
  plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
  
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in overall])
    max_index = [abs(val) for val in overall].index(max_value)
    max_value = overall[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+'\n at confidence\nthreshold = '+confidence[max_index]
  else:
    min_value = min([abs(val) for val in overall])
    min_index = [abs(val) for val in overall].index(min_value)
    min_value = overall[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index]

  anchored_text = AnchoredText(string, loc=locations[metric])
  ax.add_artist(anchored_text)
  return

ani = ['ani100', 'ani99', 'ani97', 'ani95']
species_diversity = ['cLOW', 'cHIGH']
strain_diversity = ['stFalse', 'stTrue']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
db = 'kraken2_refseqV205'
refseq_calcs = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
saving_figures = True

if saving_figures:
  for m in range(len(all_metrics)):
    metric = all_metrics[m]
    # if metric == "Simpson's diversity":
    #   do_nothing = True
    # else: continue
    plt.figure(figsize=(15,15))
    count = 0
    for an in ani:
      for spec in species_diversity:
        for strn in strain_diversity:
          this_samples = []
          for sample in samples:
            if an in sample and spec in sample and strn in sample:
              this_samples.append(sample)
          if len(this_samples) == 0: continue
          this_ax = plt.subplot(4,4,count+1)
          plt.sca(this_ax)
          this_ax.set_title(an.replace('ani', 'ANI')+'\nSpecies diversity = '+spec.replace('c', '')+'\nStrain diversity = '+strn.replace('st', ''))
          get_plot_single_metric(this_ax, refseq_calcs, truth_calcs, this_samples, metric)
          if count > 9: this_ax.set_xlabel('Confidence threshold')
          if count in [0, 4, 8, 12]: this_ax.set_ylabel(metric)
          if metric not in limits:
            plt.ylim([-0.05, 1.05])
          else:
            #plt.ylim(limits[metric])
            plt.ylim([-0.05, 0.05])
          count += 1
    
    plt.tight_layout()    
    plt.savefig(direc_save+'figures/sample_characteristics/'+metric.replace(' ', '_')+'.png', dpi=600, bbox_inches='tight')
    plt.close()
```

### Proportion classified

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Proportion_classified.png)

### Precision taxa

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Precision_taxa.png)

### Recall taxa

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Recall_taxa.png)

### F1 score taxa

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/F1_score_taxa.png)

### Precision reads

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Precision_reads.png)

### Recall reads

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Recall_reads.png)

### F1 score reads

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/F1_score_reads.png)

### Mean F1 score

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Mean_F1_score.png)

### Simpson's diversity

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Simpson's_diversity.png)

### Shannon diversity

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Shannon_diversity.png)

### Faith's phylogenetic diversity

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Faith's_phylogenetic_diversity.png)

### Chao1 richness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Chao1_richness.png)

### McIntosh's evenness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/McIntosh's_evenness.png)

### Pielou evenness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Pielou_evenness.png)

### Simpson's evenness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Simpson's_evenness.png)

### L1 distance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/L1_distance.png)

### Robust Aitchisons distance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Robust_Aitchisons_distance.png)

### Bray-Curtis dissimilarity relative abundance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Bray-Curtis_dissimilarity_relative_abundance.png)

### Weighted unifrac distance relative abundance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Weighted_unifrac_distance_relative_abundance.png)

### Unweighted unifrac distance relative abundance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Unweighted_unifrac_distance_relative_abundance.png)

### Aitchisons distance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Aitchisons_distance.png)

### Bray-Curtis dissimilarity raw

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Bray-Curtis_dissimilarity_raw.png)

### Weighted unifrac distance raw

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Weighted_unifrac_distance_raw.png)

### Unweighted unifrac distance raw

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics/Unweighted_unifrac_distance_raw.png)

## Different sample characteristics confidence 0.65 {.tabset}

## Reduced metrics

```{python, results='hide', fig.keep='all' }
ani = ['ani100', 'ani99', 'ani97', 'ani95']
species_diversity = ['cLOW', 'cHIGH']
strain_diversity = ['stFalse', 'stTrue']
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
db = 'kraken2_refseqV205'
refseq_calcs = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
limited_limits = {'Proportion classified':[0.935, 1], 'Mean F1 score':[0.4, 0.9], "Simpson's diversity":[-0.02, 0.05], 'L1 distance':[400000, 16000000]}
saving_figures = True
y_labels = ['Proportion', 'Score', 'Diversity', 'Distance']

plt.figure(figsize=(20,8))

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  count = 0
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  labels, yloc = [], []
  for strain in strain_diversity:
    for species in species_diversity:
      for an in ani:
        all_vals = []
        these_samples = []
        for sample in samples:
          if an in sample and species in sample and strain in sample: these_samples.append(sample)
        print(these_samples)
        for sample in these_samples:
          if an in sample and species in sample and strain in sample:
            try:
              val = refseq_calcs.loc[sample+'-'+db+'-'+'0.65', metric]
            except:
              if metric in prec_rec_f1 or metric in alpha_div: val = 0
              else: val = max(refseq_calcs.loc[:, metric].values)
            if metric in alpha_div: val = val-truth_calcs.loc[sample, metric]
            all_vals.append(val)
        if len(all_vals) == 0: continue
        labels.append(an.replace('ani', 'ANI')+', Species diversity = '+species.replace('c', '')+'\nStrain diversity = '+strain.replace('st', ''))
        count += 1
        yloc.append(count)
        box = ax.boxplot(all_vals, positions=[count], showfliers=False, widths=0.5, vert=False)
        for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
        ax.scatter(all_vals, np.random.normal(count, scale=0.1, size=len(all_vals)), color='r', alpha=0.5, s=2)
  if m == 0: plt.yticks(yloc, labels)
  else: plt.yticks(yloc, [])
  plt.xlim(limited_limits[metric])  
  plt.title(metric, fontweight='bold')
  plt.ylim([0.5, 14.5])
  plt.xlabel(y_labels[m])

if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_sample_characteristics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Filtering taxa

### Reduced metrics

```{python, results='hide', fig.keep='all'}
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_filtered_calculations.csv', header=0, index_col=0)
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
limited_limits = {'Proportion classified':[0.8, 1.01], 'Mean F1 score':[0, 0.9], "Simpson's diversity":[-0.025, 0.025], 'L1 distance':[40000, 12000000]}
limited_locations = {'Proportion classified':'lower left', 'Mean F1 score':'lower center', "Simpson's diversity":'lower left', 'L1 distance':'upper right'}

for a in range(len(limited_metrics)):
  ax = plt.subplot2grid((4,5),(1,a), rowspan=2)
  ax.set_title(limited_metrics[a], fontweight='bold')
  plt.sca(ax)
  all_samples = [[] for conf in confidence[1:]]
  for sample in samples:
    this_sample, this_conf = [], []
    for c in range(len(confidence[1:])):
      try:
        this_val = this_db.loc[sample+'-'+db+'-0.00_filtered_'+confidence[1:][c], limited_metrics[a]]
      except: 
        if limited_metrics[a] in alpha_div or limited_metrics[a] in beta_div:
          this_val = max(this_db.loc[:, limited_metrics[a]].values)
        else:
          this_val = 0
      if limited_metrics[a] in alpha_div:
        this_val = this_val-truth_calcs.loc[sample, limited_metrics[a]]
      this_sample.append(this_val)
      all_samples[c].append(this_val)
      this_conf.append(float(confidence[1:][c]))
    ax.plot(this_conf, this_sample, 'k-', alpha=0.05)
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence[1:]], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence[1:]], upper, lower, color='firebrick', alpha=0.2)
  plt.xticks([float(conf) for conf in confidence[1:]], confidence[1:], rotation=90)
  plt.xlabel('Confidence threshold')
  plt.xlim([-0.025, 1.025])
  
  if limited_metrics[a] in prec_rec_f1:
    max_value = max([abs(val) for val in overall])
    max_index = [abs(val) for val in overall].index(max_value)
    max_value = overall[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+'\n at confidence\nthreshold = '+confidence[1:][max_index]
  else:
    min_value = min([abs(val) for val in overall])
    min_index = [abs(val) for val in overall].index(min_value)
    min_value = overall[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[1:][min_index]

  anchored_text = AnchoredText(string, loc=limited_locations[limited_metrics[a]])
  ax.add_artist(anchored_text)
  plt.ylim(limited_limits[limited_metrics[a]])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_filter_reduced_metrics_lines.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics together

```{python, results='hide', fig.keep='all'}
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_filtered_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,35))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', header=0, index_col=0)

c = 1
axes  = []
for a in range(7):
  for b in range(4):
    if a == 3 and b == 3: continue
    if a == 6 and b > 0: continue
    ax = plt.subplot2grid((32,4), (c,b), rowspan=4)
    axes.append(ax)
  c += 4
  if a in [1,3]: c += 1
  
axes[0].text(0, 1.2, 'A     Proportion of reads classified, precision, recall and F1 score', fontweight='bold', ha='left', va='center', fontsize=16, transform=axes[0].transAxes)
axes[8].text(0, 1.2, 'B     Alpha diversity, richness and evenness', fontweight='bold', ha='left', va='center', fontsize=16, transform=axes[8].transAxes)
axes[15].text(0, 1.2, 'C     Beta diversity', fontweight='bold', ha='left', va='center', fontsize=16, transform=axes[15].transAxes)

for m in range(len(all_metrics_together)):
  metric = all_metrics_together[m]
  ax = axes[m]
  plt.sca(ax)
  ax.set_title(metric.replace('Proportion classified', 'Proportion of reads classified').replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw').replace('phylogenetic', 'taxonomic').replace('unifrac distance', 'unifrac taxonomic distance'), fontweight='bold')

  all_samples = [[] for conf in confidence[1:]]
  for sample in samples:
    this_sample, this_conf = [], []
    for c in range(len(confidence[1:])):
      try:
        this_val = this_db.loc[sample+'-'+db+'-0.00_filtered_'+confidence[1:][c], metric]
      except:
        this_val = 0
      if metric in alpha_div or metric in beta_div:
        if this_val == 0:
          this_val = max(this_db.loc[:, metric].values)
      if metric in alpha_div:
        this_val = this_val-truth_calcs.loc[sample, metric]
      this_sample.append(this_val)
      all_samples[c].append(this_val)
      this_conf.append(float(confidence[1:][c]))
    ax.plot(this_conf, this_sample, 'k-', alpha=0.05)
  overall, upper, lower = [], [], []
  for b in range(len(all_samples)):
    overall.append(np.median(all_samples[b]))
    upper.append(np.percentile(all_samples[b], 75))
    lower.append(np.percentile(all_samples[b], 25))
  ax.plot([float(conf) for conf in confidence[1:]], overall, color='firebrick')
  ax.fill_between([float(conf) for conf in confidence[1:]], upper, lower, color='firebrick', alpha=0.2)
  if m in [4,5,6,7,11,12,13,14,20,21,22,23]:
    plt.xticks([float(conf) for conf in confidence[1:]], confidence[1:], rotation=90)
    plt.xlabel('Confidence threshold taxa filter')
  else: plt.xticks([float(conf) for conf in confidence[1:]], ['' for conf in confidence[1:]])
  plt.xlim([-0.025, 1.025])

  if metric in limits_all:
    plt.ylim(limits[metric])
  elif metric in prec_rec_f1:
    plt.ylim([-0.05, 1.05])

  if metric not in alpha_div and metric not in beta_div:
    plt.ylabel(y_labels[metric], fontsize=8)
  elif m in [8,12,15,19,23]:
    plt.ylabel(y_labels[metric], fontsize=8)

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in overall])
    max_index = [abs(val) for val in overall].index(max_value)
    max_value = overall[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+'\nat confidence\nthreshold = '+confidence[1:][max_index]
  else:
    min_value = min([abs(val) for val in overall])
    min_index = [abs(val) for val in overall].index(min_value)
    min_value = overall[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+'\nat confidence\nthreshold = '+confidence[1:][min_index]

  anchored_text = AnchoredText(string, loc=locations[metric])
  ax.add_artist(anchored_text)

#plt.tight_layout()
plt.subplots_adjust(hspace=3)
if saving_figures:
  plt.savefig(direc_save+'figures/RefSeqV205_confidence_filter_all_metrics_lines.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

# 4. Comparison of other databases at all confidence thresholds

```{python, results='hide', fig.keep='all'}
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)

def plot_lines(this_db, plot_columns):
  for a in range(len(plot_columns)):
    if plot_columns == prec_rec_f1:
      if a == 0:
        ax = plt.subplot2grid((4,5),(1,0), rowspan=2)
        plt.sca(ax)
        plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
        plt.xlabel('Confidence threshold')
      elif a == 7:
        ax = plt.subplot2grid((4,5),(1,4), rowspan=2)
        plt.sca(ax)
        plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
        plt.xlabel('Confidence threshold')
      elif a > 3:
        ax = plt.subplot2grid((4,5), (2,a-3), rowspan=2)
        plt.sca(ax)
        plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
        plt.xlabel('Confidence threshold')
      else:
        ax = plt.subplot2grid((4,5), (0,a), rowspan=2)
        plt.sca(ax)
        plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence])
        
    elif plot_columns == alpha_div:
      ax = plt.subplot(2,4,a+1)
      plt.sca(ax)
      if a > 2:
        plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
        plt.xlabel('Confidence threshold')
      else:
        plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence])
    elif plot_columns == beta_div:
      if a == 0:
        ax = plt.subplot2grid((4,5),(1,0), rowspan=2)
        plt.sca(ax)
        plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
        plt.xlabel('Confidence threshold')
      elif a > 4:
        ax = plt.subplot2grid((4,5), (2,a-4), rowspan=2)
        plt.sca(ax)
        plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
        plt.xlabel('Confidence threshold')
      else:
        ax = plt.subplot2grid((4,5), (0,a), rowspan=2)
        plt.sca(ax)
        plt.xticks([float(conf) for conf in confidence], ['' for conf in confidence])
    ax.set_title(plot_columns[a].replace('relative abundance', '\nrelative abundance').replace('raw', '\n raw'), fontweight='bold')
    all_samples = [[] for conf in confidence]
    for sample in samples:
      this_sample = []
      for c in range(len(confidence)):
        try:
          this_val = this_db.loc[sample+'-'+db+'-'+confidence[c], plot_columns[a]]
        except: 
          if plot_columns[a] in prec_rec_f1 or plot_columns[a] in alpha_div:
            this_val = 0
          else:
            this_val = max(this_db.loc[:, plot_columns[a]].values)
        if plot_columns[a] in alpha_div:
          this_val = this_val-truth_calcs.loc[sample, plot_columns[a]]
        this_sample.append(this_val)
        all_samples[c].append(this_val)
      ax.plot([float(conf) for conf in confidence], this_sample, 'k-', alpha=0.05)
    overall, upper, lower = [], [], []
    for b in range(len(all_samples)):
      #ax.scatter(np.random.normal(float(confidence[b]), scale=0.01, size=len(all_samples[b])), all_samples[b], s=2, alpha=0.05, color='k')
      overall.append(np.median(all_samples[b]))
      upper.append(np.percentile(all_samples[b], 75))
      lower.append(np.percentile(all_samples[b], 25))
    ax.plot([float(conf) for conf in confidence], overall, color='firebrick')
    ax.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
    
    if plot_columns[a] in prec_rec_f1:
      max_value = max([abs(val) for val in overall])
      max_index = [abs(val) for val in overall].index(max_value)
      max_value = overall[max_index]
      string = 'Maximum = '+str(round(max_value, 3))+'\n at confidence\nthreshold = '+confidence[max_index]
    else:
      min_value = min([abs(val) for val in overall])
      min_index = [abs(val) for val in overall].index(min_value)
      min_value = overall[min_index]
      string = 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index]
  
    anchored_text = AnchoredText(string, loc=locations[plot_columns[a]])
    ax.add_artist(anchored_text)
    
    if plot_columns[a] in alpha_div:
      ax.plot([float(conf) for conf in confidence], [0 for conf in confidence], 'k--')
    plt.xlim([-0.025, 1.025])
    if plot_columns[a] not in prec_rec_f1:
      plt.ylim(limits[plot_columns[a]])
    else:
      plt.ylim([-0.05, 1.05])
  return
```

## ChocoPhlAn 3 equivalent {.tabset}

### Median lines precision, recall, F1

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1
db = 'kraken2_chocophlan'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines alpha diversity

```{python, results='hide', fig.keep='all' }
plot_columns=alpha_div
db = 'kraken2_chocophlan'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(15,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines beta diversity

```{python, results='hide', fig.keep='all' }
plot_columns=beta_div
db = 'kraken2_chocophlan'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

## Standard (05/2021) database {.tabset}

### Median lines precision, recall, F1

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1
db = 'kraken2_standard_0521'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines alpha diversity

```{python, results='hide', fig.keep='all' }
plot_columns=alpha_div
db = 'kraken2_standard_0521'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(15,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines beta diversity

```{python, results='hide', fig.keep='all' }
plot_columns=beta_div
db = 'kraken2_standard_0521'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```


## MiniKraken V2 {.tabset}

### Median lines precision, recall, F1

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1
db = 'kraken2_minikraken'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines alpha diversity

```{python, results='hide', fig.keep='all' }
plot_columns=alpha_div
db = 'kraken2_minikraken'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(15,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines beta diversity

```{python, results='hide', fig.keep='all' }
plot_columns=beta_div
db = 'kraken2_minikraken'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```


## NCBI RefSeq Complete V205 100GB {.tabset}

### Median lines precision, recall, F1

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1
db = 'kraken2_refseqV205_100GB'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines alpha diversity

```{python, results='hide', fig.keep='all' }
plot_columns=alpha_div
db = 'kraken2_refseqV205_100GB'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(15,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines beta diversity

```{python, results='hide', fig.keep='all' }
plot_columns=beta_div
db = 'kraken2_refseqV205_100GB'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

## NCBI RefSeq Complete V205 500GB {.tabset}

### Median lines precision, recall, F1

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1
db = 'kraken2_refseqV205_500GB'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines alpha diversity

```{python, results='hide', fig.keep='all' }
plot_columns=alpha_div
db = 'kraken2_refseqV205_500GB'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(15,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines beta diversity

```{python, results='hide', fig.keep='all' }
plot_columns=beta_div
db = 'kraken2_refseqV205_500GB'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

## GTDB r202 + NCBI RefSeq Complete V205 {.tabset}

### Median lines precision, recall, F1

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1
db = 'kraken2_GTDBr202RefSeqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines alpha diversity

```{python, results='hide', fig.keep='all' }
plot_columns=alpha_div
db = 'kraken2_GTDBr202RefSeqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(15,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines beta diversity

```{python, results='hide', fig.keep='all' }
plot_columns=beta_div
db = 'kraken2_GTDBr202RefSeqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

## NCBI RefSeq nt V208 {.tabset}

### Median lines precision, recall, F1

```{python, results='hide', fig.keep='all' }
plot_columns=prec_rec_f1
db = 'kraken2_refseqV208_nt'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines alpha diversity

```{python, results='hide', fig.keep='all' }
plot_columns=alpha_div
db = 'kraken2_refseqV208_nt'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(15,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

### Median lines beta diversity

```{python, results='hide', fig.keep='all' }
plot_columns=beta_div
db = 'kraken2_refseqV208_nt'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))

plot_lines(this_db, plot_columns)

plt.figtext(0.5, 0.99, rename_db[db]+'\n', fontweight='bold', fontsize=16, ha='center', va='bottom')
plt.tight_layout()
plt.show()
```

# 5. Minimizer filtering comparison with NCBI RefSeq V205 only {.tabset}

## Distribution of minimizers across samples

Look at number of minimizers and number of distinct minimizers for true positive vs false positive taxa in samples.
In total there are 49,099 true positive and 1,535,534 false positive taxa.
```{python, results='hide', fig.keep='all'}
with open(direc_db+'Minimizer_info.dict', 'rb') as f:
    minimizers = pickle.load(f)
#this is a dictionary of dictionaries, accessed by minimizers[sample name][taxonomy id (integer)]

truth = pd.read_csv(direc_db+'truth_rename_reads.csv', index_col=0, header=0)
truth.index = truth.index.map(int)
plt.figure(figsize=(10,10))
ax1 = plt.subplot(111)

colormap = mpl.cm.get_cmap('viridis', 256)
norm = mpl.colors.Normalize(vmin=10, vmax=100000)
m = mpl.cm.ScalarMappable(norm=norm, cmap=colormap)

all_true, all_false = [], []
count = 0
for sample in truth.columns:
  # count += 1
  # if count > 10: break
  true_positive, false_positive = [], []
  if sample in minimizers:
    for tax in minimizers[sample]:
      distinct = minimizers[sample][tax][1]
      if tax in truth.index.values:
        if truth.loc[tax, sample] > 0:
          true_positive.append(distinct)
          #ax1.scatter(np.random.normal(1, scale=0.1, size=1), [distinct], color=m.to_rgba(truth.loc[tax, sample]), s=2, alpha=0.3)
        else:
          false_positive.append(distinct)
      else:
        false_positive.append(distinct)
  ax1.scatter(np.random.normal(1, scale=0.1, size=len(true_positive)), true_positive, color='b', s=2, alpha=0.3)
  ax1.scatter(np.random.normal(2, scale=0.1, size=len(false_positive)), false_positive, color='r', s=2, alpha=0.3)
  all_true = all_true+true_positive
  all_false = all_false+false_positive

print(len(all_true), len(all_false))
box = ax1.boxplot([all_true, all_false], positions=[1, 2], widths=0.8, showfliers=False)
for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

# handles = [Patch(facecolor=m.to_rgba(float(conf)), edgecolor='k', label=conf) for conf in confidence]
# axes[3].legend(handles=handles, bbox_to_anchor=(1.05,1.05), loc='upper left')

plt.semilogy()
plt.xticks([1, 2], ['True positives\n'+str(len(all_true))+' taxa', 'False positives\n'+str(len(all_false))+' taxa'])
plt.ylabel('Number of distinct minimizers \n(shown separately for each taxon in each sample)')

if saving_figures:
  plt.savefig(direc_save+'figures/minimizers_in_true_and_false_positive_taxa.png', bbox_inches='tight', dpi=600)
  plt.show()
else:
  plt.show()
```

## Comparison with confidence 0, 0.5, 0.9 and 1 {.tabset}

Added in 0.9 because it is where the F1 score is maximised for the RefSeq Complete database.

### Precision, recall, F1 score

```{python, results='hide', fig.keep='all' }
db = 'kraken2_refseqV205'
this_db = pd.read_csv(direc_save+db+'_calculations.csv', header=0, index_col=0)
fig = plt.figure(figsize=(20,10))
names = ['Confidence\nthreshold', 'Minimizer\nfiltering']

axes = []
for a in range(8):
  if a == 0:
    ax = plt.subplot2grid((4,5),(1,0), rowspan=2)
  elif a == 7:
    ax = plt.subplot2grid((4,5),(1,4), rowspan=2)
  elif a > 3:
    ax = plt.subplot2grid((4,5), (2,a-3), rowspan=2)
  else:
    ax = plt.subplot2grid((4,5), (0,a), rowspan=2)
  ax.set_title(prec_rec_f1[a], fontweight='bold')
  axes.append(ax)
  
mi1, mi2, mi3, mi4, mi5 = '100', '500', '1000', '2500', '5000'

refseq = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', header=0, index_col=0)
mini = pd.read_csv(direc_save+'kraken2_refseqV205_minimizers_calculations.csv', header=0, index_col=0)
rename_refseq, rename_mini = {}, {}
for row in refseq.index.values: rename_refseq[row] = row.split('-')[-1]
for row in mini.index.values: rename_mini[row] = row.split('-')[-1]
refseq = refseq.rename(index=rename_refseq)
mini = mini.rename(index=rename_mini)

c1, c2, c3, c4 = '#154360', '#2980B9', '#3498DB', '#5DADE2'
c5, c6, c7, c8, c9 = '#922B21', '#D35400', '#E67E22', '#F39C12', '#F1C40F'

for a in range(8):
  refseq_0 = list(refseq.loc['0.00', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(1, scale=0.1, size=len(refseq_0)), refseq_0, s=2, alpha=0.1, color=c1)

  refseq_05 = list(refseq.loc['0.50', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(2, scale=0.1, size=len(refseq_05)), refseq_05, s=2, alpha=0.1, color=c2)

  refseq_06 = list(refseq.loc['0.65', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(3, scale=0.1, size=len(refseq_06)), refseq_06, s=2, alpha=0.1, color=c3)

  refseq_1 = list(refseq.loc['1.00', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(4, scale=0.1, size=len(refseq_1)), refseq_1, s=2, alpha=0.1, color=c4)

  mini_1 = list(mini.loc['100minimizer', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(6, scale=0.1, size=len(mini_1)), mini_1, s=2, alpha=0.1, color=c5)

  mini_2 = list(mini.loc['500minimizer', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(7, scale=0.1, size=len(mini_2)), mini_2, s=2, alpha=0.1, color=c6)

  mini_3 = list(mini.loc['1000minimizer', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(8, scale=0.1, size=len(mini_3)), mini_3, s=2, alpha=0.1, color=c7)

  mini_4 = list(mini.loc['2500minimizer', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(9, scale=0.1, size=len(mini_4)), mini_4, s=2, alpha=0.1, color=c8)

  mini_5 = list(mini.loc['5000minimizer', prec_rec_f1[a]].values)
  axes[a].scatter(np.random.normal(10, scale=0.1, size=len(mini_5)), mini_5, s=2, alpha=0.1, color=c9)

  box = axes[a].boxplot([refseq_0, refseq_05, refseq_06, refseq_1, mini_1, mini_2, mini_3, mini_4, mini_5], positions=[1, 2, 3, 4, 6, 7, 8, 9, 10], showfliers=False, widths=0.8)
  plt.sca(axes[a])
  for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')

for a in range(8):
  plt.sca(axes[a])
  if a == 0 or a > 3:
    plt.xticks([2.5, 8], names)
  else:
    plt.xticks([2.5, 8], ['' for b in range(2)])
  if a == 0:
    plt.ylim([0.75, 1.05])

colors, names = [c1, c2, c3, c4, c5, c6, c7, c8, c9], ['Confidence=0.00', 'Confidence=0.50', 'Confidence=0.65', 'Confidence=1.00', mi1+'+ Minimizers', mi2+'+ Minimizers', mi3+'+ Minimizers', mi4+'+ Minimizers', mi5+'+ Minimizers']
handles = [Patch(facecolor=colors[a], edgecolor='k', label=names[a]) for a in range(len(colors))]
axes[7].legend(handles=handles, bbox_to_anchor=(0.5,1.1), loc='lower center')

#plt.tight_layout()
plt.subplots_adjust(hspace=0.3, wspace=0.2)
if saving_figures:
  plt.savefig(direc_save+'figures/kraken_confidence_vs_minimizer_'+mi1+'_'+mi2+'_'+mi3+'_'+mi4+'_'+mi5+'.png', bbox_inches='tight', dpi=600)
  plt.show()
else:
  plt.show()
# plt.close()
```

# 6. Comparison of all databases {.tabset}

## All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
all_dbs = ['kraken2_minikraken', 'kraken2_standard_0521', 'kraken2_chocophlan', 'kraken2_refseqV205_100GB', 'kraken2_refseqV208_nt', 'kraken2_refseqV205_500GB', 'kraken2_GTDBr202RefSeqV205', 'kraken2_refseqV205']
all_dbs.reverse()
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  #if m > 2: continue
  all_medians = []
  for db in all_dbs:
    this_median = []
    rename_samples = {}
    this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
    for row in this_db.index.values:
      rename_samples[row] = row.split('-')[2]
    this_db = this_db.rename(index=rename_samples)
    for c in range(len(confidence)):
      val = np.median(this_db.loc[confidence[c], metric].values)
      if metric in alpha_div:
        val = val-np.median(truth_calcs.loc[:, metric].values)
      this_median.append(val)
    all_medians.append(this_median)
  this_metric = pd.DataFrame(all_medians, index=all_dbs, columns=confidence)
  all_vals = []
  for val_set in list(this_metric.values):
    all_vals += list(val_set)

  lq = np.percentile(all_vals, 25)
  uq = np.percentile(all_vals, 75)
  
  cmap = 'viridis'
  if metric in prec_rec_f1: 
    cmap = 'plasma'
  elif metric in alpha_div:
    cmap = 'bwr'
    new_lq = -(max(abs(lq), abs(uq)))
    new_uq = max(abs(lq), abs(uq))
    lq, uq = new_lq, new_uq
  
  heatmap = plt.pcolor(this_metric, edgecolor='k', cmap=cmap, vmin=lq, vmax=uq)
  cbar = plt.colorbar(heatmap)
  
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  if m >= 18:
    plt.xticks([c+0.5 for c in range(len(confidence))], confidence, rotation=90)
    plt.xlabel('Confidence threshold')
  else:
    plt.xticks([c+0.5 for c in range(len(confidence))], [], rotation=90)
  
  if m in [0, 6, 12, 18]:
    plt.yticks([a+0.5 for a in range(len(all_dbs))], [rename_db[db] for db in all_dbs])
  else:
    plt.yticks([a+0.5 for a in range(len(all_dbs))], [])
  plt.ylim([0, len(all_dbs)]), plt.xlim([0, 21])
  

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/confidence_comparison_all.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Selection of metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(34,15))
all_dbs = ['kraken2_minikraken', 'kraken2_standard_0521', 'kraken2_chocophlan', 'kraken2_refseqV205_100GB', 'kraken2_refseqV208_nt', 'kraken2_refseqV205_500GB', 'kraken2_GTDBr202RefSeqV205', 'kraken2_refseqV205']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
limited_limits = {'Proportion classified':[-0.05, 1.05], 'Mean F1 score':[0, 0.9], "Simpson's diversity":[-1, 0.25], 'L1 distance':[400000, 16000000]}
limited_locations = ['upper right', 'lower left', 'lower left', 'upper right', 'lower left', 'upper right', 'lower left', 'lower left',
             'upper right', 'upper left', 'upper left', 'upper right', 'upper left', 'upper right', 'upper left', 'lower right',
             'lower left', 'lower left', 'lower left', 'upper right', 'lower left', 'lower left', 'lower left', 'lower left',
             'lower right', 'upper left', 'upper left', 'lower right', 'lower right', 'lower right', 'lower right', 'upper left']

count = 0
for metric in limited_metrics:
  for db in all_dbs:
    count += 1
    ax = plt.subplot(4,8,count)
    plt.sca(ax)
    if db == 'kraken2_minikraken': plt.ylabel(metric.replace('Proportion classified', 'Proportion of reads classified'), fontweight='bold')
    if metric == 'Proportion classified': plt.title(rename_db[db].replace('Complete', '\nComplete'), fontweight='bold')
    if metric == 'L1 distance': 
      plt.xticks([float(conf) for conf in confidence], confidence, rotation=90)
      plt.xlabel('Confidence threshold')
    else: plt.xticks([float(conf) for conf in confidence], [])
    
    this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
    all_conf = [[] for conf in confidence]
    for sample in samples:
      this_sample = []
      for c in range(len(confidence)):
        conf = confidence[c]
        try:
          val = this_db.loc[sample+'-'+db+'-'+conf, metric]
        except:
          if metric in prec_rec_f1: val = 0
          else: val = max(this_db.loc[:, metric].values)
        if metric in alpha_div: val = val-truth_calcs.loc[sample, metric]
        this_sample.append(val)
        all_conf[c].append(val)
      line = plt.plot([float(conf) for conf in confidence], this_sample, 'k', alpha=0.05)
    overall, upper, lower = [], [], []
    for b in range(len(all_conf)):
      overall.append(np.median(all_conf[b]))
      upper.append(np.percentile(all_conf[b], 75))
      lower.append(np.percentile(all_conf[b], 25))
    line = plt.plot([float(conf) for conf in confidence], overall, color='firebrick')
    line = plt.fill_between([float(conf) for conf in confidence], upper, lower, color='firebrick', alpha=0.2)
    plt.ylim(limited_limits[metric])
    
    if metric in prec_rec_f1:
      max_value = max([abs(val) for val in overall])
      max_index = [abs(val) for val in overall].index(max_value)
      max_value = overall[max_index]
      string = 'Maximum = '+str(round(max_value, 3))+'\n at confidence\nthreshold = '+confidence[max_index]
    else:
      min_value = min([abs(val) for val in overall])
      min_index = [abs(val) for val in overall].index(min_value)
      min_value = overall[min_index]
      string = 'Minimum = '+str(round(min_value, 3))+'\n at confidence\nthreshold = '+confidence[min_index]

    anchored_text = AnchoredText(string, loc=limited_locations[count-1])
    ax.add_artist(anchored_text)

if saving_figures:
  plt.savefig(direc_save+'figures/confidence_comparison_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

# 7. Comparison of NCBI RefSeq V205 with reduced databases (100GB, 500GB and RefSeq V208 nt) {.tabset}

I don't think any of these are really necessary with having the above ones already. 

## Selection of metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,20))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
limit_conf = ['0.00', '0.50', '1.00', 'Maximum mean F1 score']
all_dbs = ['kraken2_refseqV205_100GB', 'kraken2_refseqV208_nt', 'kraken2_refseqV205_500GB', 'kraken2_refseqV205']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
location_db = {'kraken2_refseqV205_100GB':1, 'kraken2_refseqV208_nt':2, 'kraken2_refseqV205_500GB':3, 'kraken2_refseqV205':4}
limited_limits = {'Proportion classified':[-0.05, 1.05], 'Mean F1 score':[-0.05, 1.05], "Simpson's diversity":[-1, 0.25], 'L1 distance':[400000, 18000000]}

max_f1 = {}
for db in all_dbs:
  median = []
  this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
  for conf in confidence:
    this_conf = []
    for sample in samples:
      try:
        this_conf.append(this_db.loc[sample+'-'+db+'-'+conf, 'Mean F1 score'])
      except:
        this_conf.append(0)
    median.append(np.median(this_conf))
  
  max_value = max(median)
  max_index = median.index(max_value)
  max_conf = confidence[max_index]
  max_f1[db] = max_conf


count = 0
for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  for c in range(len(limit_conf)):
    using_conf = limit_conf[c]
    ax = plt.subplot(4,4,count+1)
    plt.sca(ax)
    if c == 0:
      plt.ylabel(metric, fontweight='bold')
    if m == 0:
      plt.title('Confidence threshold = '+using_conf, fontweight='bold')
    
    for db in all_dbs:
      this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
      all_vals = []
      for sample in samples:
        try:
          if limit_conf[c] in confidence:
            val = this_db.loc[sample+'-'+db+'-'+using_conf, metric]
          else:
            val = this_db.loc[sample+'-'+db+'-'+max_f1[db], metric]
        except:
          if metric in prec_rec_f1 or metric in alpha_div:
            val = 0
          else:
            val = max(this_db.loc[:, metric].values)
        if metric in alpha_div:
          val = val-truth_calcs.loc[sample, metric]
        all_vals.append(val)
      box = ax.boxplot(all_vals, positions=[location_db[db]], showfliers=False, widths=0.5)
      for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
      ax.scatter(np.random.normal(location_db[db], scale=0.1, size=len(all_vals)), all_vals, color=colors_db[db], alpha=0.1, s=2)
      plt.ylim(limited_limits[metric])
    
    if m == 3:
      if c == 3: plt.xticks([1, 2, 3, 4], [rename_db[db]+'\nConfidence threshold = '+max_f1[db] for db in all_dbs], rotation=90)
      else: plt.xticks([1, 2, 3, 4], [rename_db[db] for db in all_dbs], rotation=90)
    else:
      plt.xticks([1, 2, 3, 4], '', rotation=90)
    count += 1

if saving_figures:
  plt.savefig(direc_save+'figures/database_comparison_reduced.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## All at confidence = 0

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
all_dbs = ['kraken2_refseqV205_100GB', 'kraken2_refseqV208_nt', 'kraken2_refseqV205_500GB', 'kraken2_refseqV205']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
location_db = {'kraken2_refseqV205_100GB':1, 'kraken2_refseqV208_nt':2, 'kraken2_refseqV205_500GB':3, 'kraken2_refseqV205':4}

limits = {"Proportion classified":[0.8, 1.01], "Simpson's diversity":[-0.25, 0.25], "Shannon diversity":[-2.5, 1.25], "Faith's phylogenetic diversity":[-2000, 20000], "Chao1 richness":[-2000, 20000], "McIntosh's evenness":[-0.3, 0.3], "Pielou evenness":[-0.5, 0.25], "Simpson's evenness": [-0.3, 0.15], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 50], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

limits['Shannon diversity'] = [-1.5, 2]
limits["Simpson's diversity"] = [-0.1, 0.1]
limits["Faith's phylogenetic diversity"] = [-2000, 30000]
limits["Aitchisons distance"] = [-10, 350]
limits["Unweighted unifrac distance raw"] = [0.8, 1.05]
limits["Precision taxa"] = [-0.05, 0.2]
limits["F1 score taxa"] = [-0.05, 0.2]

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  #if m > 2: continue
  for db in all_dbs:
    # rename_samples = {}
    this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
    all_vals = []
    for sample in samples:
      try:
        val = this_db.loc[sample+'-'+db+'-0.00', metric]
      except:
        if metric in prec_rec_f1 or metric in alpha_div:
          val = 0
        else:
          val = max(this_db.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      all_vals.append(val)
    box = ax.boxplot(all_vals, positions=[location_db[db]], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    ax.scatter(np.random.normal(location_db[db], scale=0.1, size=len(all_vals)), all_vals, color=colors_db[db], alpha=0.1, s=2)
  
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  if m >= 17:
    plt.xticks([1, 2, 3, 4], [rename_db[db] for db in all_dbs], rotation=90)
  else:
    plt.xticks([1, 2, 3, 4], '', rotation=90)
  
  if metric in limits:
    plt.ylim(limits[metric])
  else:
    plt.ylim([-0.05, 1.05])

  # if m in [0, 6, 12, 18]:
  #   plt.ylabel(metric)
  
#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/database_comparison_db_limit_conf_0.00.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## All at confidence = 0.5

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
metric = 'Proportion classified'
all_dbs = ['kraken2_refseqV205_100GB', 'kraken2_refseqV208_nt', 'kraken2_refseqV205_500GB', 'kraken2_refseqV205']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
location_db = {'kraken2_refseqV205_100GB':1, 'kraken2_refseqV208_nt':2, 'kraken2_refseqV205_500GB':3, 'kraken2_refseqV205':4}

limits = {"Proportion classified":[0.8, 1.01], "Simpson's diversity":[-0.25, 0.25], "Shannon diversity":[-2.5, 1.25], "Faith's phylogenetic diversity":[-2000, 20000], "Chao1 richness":[-2000, 20000], "McIntosh's evenness":[-0.3, 0.3], "Pielou evenness":[-0.5, 0.25], "Simpson's evenness": [-0.3, 0.15], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 50], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

limits["Proportion classified"] = [-0.05, 1.01]
limits["Faith's phylogenetic diversity"] = [-2000, 2500]
limits["Chao1 richness"] = [-2000, 2500]
limits["McIntosh's evenness"] = [-0.5, 0.3]
limits["Pielou evenness"] = [-1.05, 0.25]
limits["Shannon diversity"] = [-8.1, 1.25]
limits["Simpson's diversity"] = [-1.05, 0.25]
limits["Weighted unifrac distance relative abundance"] = [-10, 10000]


for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  #if m > 2: continue
  for db in all_dbs:
    # rename_samples = {}
    this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
    all_vals = []
    for sample in samples:
      try:
        val = this_db.loc[sample+'-'+db+'-0.50', metric]
      except:
        if metric in prec_rec_f1 or metric in alpha_div:
          val = 0
        else:
          val = max(this_db.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      all_vals.append(val)
    box = ax.boxplot(all_vals, positions=[location_db[db]], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    ax.scatter(np.random.normal(location_db[db], scale=0.1, size=len(all_vals)), all_vals, color=colors_db[db], alpha=0.1, s=2)
  
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  if m >= 17:
    plt.xticks([1, 2, 3, 4], [rename_db[db] for db in all_dbs], rotation=90)
  else:
    plt.xticks([1, 2, 3, 4], '', rotation=90)
  
  if metric in limits:
    plt.ylim(limits[metric])
  else:
    plt.ylim([-0.05, 1.05])

  # if m in [0, 6, 12, 18]:
  #   plt.ylabel(metric)
  
#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/database_comparison_db_limit_conf_0.50.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## All at confidence = maximum F1

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
metric = 'Proportion classified'
all_dbs = ['kraken2_refseqV205_100GB', 'kraken2_refseqV208_nt', 'kraken2_refseqV205_500GB', 'kraken2_refseqV205']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
location_db = {'kraken2_refseqV205_100GB':1, 'kraken2_refseqV208_nt':2, 'kraken2_refseqV205_500GB':3, 'kraken2_refseqV205':4}

limits = {"Proportion classified":[0.8, 1.01], "Simpson's diversity":[-0.25, 0.25], "Shannon diversity":[-2.5, 1.25], "Faith's phylogenetic diversity":[-2000, 20000], "Chao1 richness":[-2000, 20000], "McIntosh's evenness":[-0.3, 0.3], "Pielou evenness":[-0.5, 0.25], "Simpson's evenness": [-0.3, 0.15], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 50], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

limits["Faith's phylogenetic diversity"] = [-500, 1000]
limits["Chao1 richness"] = [-500, 1000]

max_f1 = {}
for db in all_dbs:
  median = []
  this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
  for conf in confidence:
    this_conf = []
    for sample in samples:
      try:
        this_conf.append(this_db.loc[sample+'-'+db+'-'+conf, 'F1 score taxa'])
      except:
        this_conf.append(0)
    median.append(np.median(this_conf))
  
  max_value = max(median)
  max_index = median.index(max_value)
  max_conf = confidence[max_index]
  max_f1[db] = max_conf

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  #if m > 2: continue
  for db in all_dbs:
    # rename_samples = {}
    this_db = pd.read_csv(direc_save+db+'_calculations.csv', index_col=0, header=0)
    all_vals = []
    for sample in samples:
      try:
        val = this_db.loc[sample+'-'+db+'-'+max_f1[db], metric]
      except:
        if metric in prec_rec_f1 or metric in alpha_div:
          val = 0
        else:
          val = max(this_db.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      all_vals.append(val)
    box = ax.boxplot(all_vals, positions=[location_db[db]], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    ax.scatter(np.random.normal(location_db[db], scale=0.1, size=len(all_vals)), all_vals, color=colors_db[db], alpha=0.1, s=2)

  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  if m >= 17:
    plt.xticks([1, 2, 3, 4], [rename_db[db]+'\nConfidence threshold = '+max_f1[db] for db in all_dbs], rotation=90)
  else:
    plt.xticks([1, 2, 3, 4], '', rotation=90)

  if metric == 'Proportion classified' or metric not in limits:
    plt.ylim([-0.05, 1.05])
  else:
    plt.ylim(limits[metric])

  # if m in [0, 6, 12, 18]:
  #   plt.ylabel(metric)

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/database_comparison_db_limit_max_f1.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

# 8. Comparison of MetaPhlAn 3 options {.tabset}

## Estimated reads, bowtie2 reads or default converted to reads

Note that for comparability the default output of MetaPhlAn 3 (relative abundances) has been multiplied by the number of reads in a sample - although this is probably not recommended in general because these relative abundances are supposed to be for community members rather than number of reads. 

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads']
setting_rename = {'default':'Default x reads in sample', 'estimated_reads':'Estimated reads', 'bowtie2_reads':'Bowtie2-mapped reads'}
locations_single = ['upper right', 'lower left', 'upper right', 'lower right']

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=10, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
    
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+setting_rename[settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+setting_rename[settings[min_index]]

  anchored_text = AnchoredText(string, loc=locations_single[m])
  ax.add_artist(anchored_text)
  
  plt.xticks([1, 2, 3], ['Default x reads in sample', 'Estimated reads', 'Bowtie2-mapped reads'], rotation=90)
  plt.xlim([0.5, 3.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_estimated_default_bowtie_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads']
setting_rename = {'default':'Default x reads in sample', 'estimated_reads':'Estimated reads', 'bowtie2_reads':'Bowtie2-mapped reads'}

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=2, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
    
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+setting_rename[settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+setting_rename[settings[min_index]]

  anchored_text = AnchoredText(string, loc='upper right')
  ax.add_artist(anchored_text)
  
  if m >= 18:
    plt.xticks([1, 2, 3], ['Default x reads in sample', 'Estimated reads', 'Bowtie2-mapped reads'], rotation=90)
  else:
    plt.xticks([1, 2, 3], [])
  
  plt.xlim([0.5, 3.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_estimated_default_bowtie_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Bowtie2 parameters

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['estimated_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local']
locations_single = ['upper right', 'lower left', 'upper right', 'lower right']

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=10, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
  
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+settings[max_index].replace('estimated_reads', 'very-sensitive').replace('_', '-')
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+settings[min_index].replace('estimated_reads', 'very-sensitive').replace('_', '-')

  anchored_text = AnchoredText(string, loc=locations_single[m])
  ax.add_artist(anchored_text)
  
  plt.xticks([1, 2, 3, 4], ['very-sensitive\n(default/same as estimated reads)', 'very-sensitive-local', 'sensitive', 'sensitive-local'], rotation=90)
  plt.xlim([0.5, 4.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_bowtie2_settings_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['estimated_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local']

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=2, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
  
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+settings[max_index].replace('estimated_reads', 'very-sensitive').replace('_', '-')
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+settings[min_index].replace('estimated_reads', 'very-sensitive').replace('_', '-')

  anchored_text = AnchoredText(string, loc='upper right')
  ax.add_artist(anchored_text)
  
  if m >= 18:
    plt.xticks([1, 2, 3, 4], ['very-sensitive\n(default/same as estimated reads)', 'very-sensitive-local', 'sensitive', 'sensitive-local'], rotation=90)
  else:
    plt.xticks([1, 2, 3, 4], [])
  
  plt.xlim([0.5, 4.5])
  if metric == 'Robust Aitchisons distance':
    plt.ylim([-2, 30])
  elif metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_bowtie2_settings_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Methods for estimating clade abundance

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['avg_g', 'avg_l', 'estimated_reads', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
locations_single = ['upper right', 'lower left', 'upper right', 'lower right']

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=10, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
  
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+settings[max_index].replace('estimated_reads', 'tavg_g (default)')
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+settings[min_index].replace('estimated_reads', 'tavg_g (default)')

  anchored_text = AnchoredText(string, loc=locations_single[m])
  ax.add_artist(anchored_text)
  
  plt.xticks([1, 2, 3, 4, 5, 6, 7], ['tavg_g (default)' if sett == 'estimated_reads' else sett for sett in settings], rotation=90)
  plt.xlim([0.5, 7.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_clade_abundance_methods_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['avg_g', 'avg_l', 'estimated_reads', 'tavg_l', 'wavg_g', 'wavg_l', 'med']

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=2, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
  
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+settings[max_index].replace('estimated_reads', 'tavg_g (default)')
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+settings[min_index].replace('estimated_reads', 'tavg_g (default)')

  anchored_text = AnchoredText(string, loc='upper right')
  ax.add_artist(anchored_text)
  
  if m >= 18:
    plt.xticks([1, 2, 3, 4, 5, 6, 7], ['tavg_g (default)' if sett == 'estimated_reads' else sett for sett in settings], rotation=90)
  else:
    plt.xticks([1, 2, 3, 4, 5, 6, 7], [])
  
  plt.xlim([0.5, 7.5])
  if metric == 'Robust Aitchisons distance':
    plt.ylim([-2, 30])
  elif metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_clade_abundance_methods_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## HUMANn output

Here we are looking at the number of reads mapped either using Bowtie2 to with the reduced ChocoPhlAn database or the Diamond translated search. Note that in both cases I have discarded reads with above two matches. This was a small percentage of reads for Bowtie2, but was much larger for Diamond. If there was only two matches and one was to the UniRef90 database while the other was to the UniRef50 database, then the UniRef90 match was kept.

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['estimated_reads', 'humann_bowtie2', 'humann_diamond']
samples_bowtie2_diamond = [pd.read_csv(direc_db+'MetaPhlAn_humann_diamond_aligned_combined_rename.csv', index_col=0, header=0), pd.read_csv(direc_db+'MetaPhlAn_humann_bowtie2_aligned_combined_rename.csv', index_col=0, header=0)]
samples_plotting = [sample.split('-')[0] for sample in samples_bowtie2_diamond[0].columns if sample.replace('diamond', 'bowtie2') in samples_bowtie2_diamond[1].columns]
setting_rename = {'humann_bowtie2':'HUMAnN Bowtie2 mapped reads', 'estimated_reads':'Estimated reads', 'humann_diamond':'HUMAnN Diamond translated mapped reads'}
locations_single = ['upper right', 'lower left', 'upper right', 'lower right']

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      if sample not in samples_plotting: continue
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=10, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+setting_rename[settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+setting_rename[settings[min_index]]

  anchored_text = AnchoredText(string, loc=locations_single[m])
  ax.add_artist(anchored_text)

  plt.xticks([1, 2, 3], ['Estimated reads', 'HUMAnN Bowtie2\nmapped reads', 'HUMAnN Diamond translated\nmapped reads'], rotation=90)
  plt.xlim([0.5, 3.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_estimated_humann_bowtie2_diamond_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = ['estimated_reads', 'humann_bowtie2', 'humann_diamond']
samples_bowtie2_diamond = [pd.read_csv(direc_db+'MetaPhlAn_humann_diamond_aligned_combined_rename.csv', index_col=0, header=0), pd.read_csv(direc_db+'MetaPhlAn_humann_bowtie2_aligned_combined_rename.csv', index_col=0, header=0)]
samples_plotting = [sample.split('-')[0] for sample in samples_bowtie2_diamond[0].columns if sample.replace('diamond', 'bowtie2') in samples_bowtie2_diamond[1].columns]
setting_rename = {'humann_bowtie2':'HUMAnN Bowtie2 mapped reads', 'estimated_reads':'Estimated reads', 'humann_diamond':'HUMAnN Diamond translated mapped reads'}

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for s in range(len(settings)):
    setting = settings[s]
    this_setting = []
    for sample in samples:
      if sample not in samples_plotting: continue
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=2, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
    
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+setting_rename[settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+setting_rename[settings[min_index]]

  anchored_text = AnchoredText(string, loc='upper right')
  ax.add_artist(anchored_text)
  
  if m >= 18:
    plt.xticks([1, 2, 3], ['Estimated reads', 'HUMAnN Bowtie2\nmapped reads', 'HUMAnN Diamond translated\nmapped reads'], rotation=90)
  else:
    plt.xticks([1, 2, 3], [])
  
  plt.xlim([0.5, 3.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_estimated_humann_bowtie2_diamond_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## All MetaPhlAn 3 comparison

```{python, results='hide', fig.keep='all'}
plt.figure(figsize=(20,15))
group_settings = [['default', 'estimated_reads'], ['estimated_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local'], ['avg_g', 'avg_l', 'estimated_reads', 'tavg_l', 'wavg_g', 'wavg_l', 'med'], ['estimated_reads', 'humann_bowtie2', 'humann_diamond']]
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
limited_metrics = ['Proportion classified', 'Mean F1 score', 'L1 distance']
ylabels = ['Default $vs$ estimated', 'Bowtie2 mapping options', 'Statistical options for\nestimating number of reads', 'HUMAnN 3 mapped reads']

colors = [['k', 'r'], ['r', 'k', 'k', 'k'], ['k', 'k', 'r', 'k', 'k', 'k', 'k'], ['r', 'k', 'k']]

r = 0
g = 0
for settings in group_settings:
  c = 0
  ns = len(settings)
  axes = []
  for metric in limited_metrics:
    ax = plt.subplot2grid((17,4),(r,c), rowspan=ns)
    plt.sca(ax)
    axes.append(ax)
    c += 1
    
    if r == 0: plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw').replace('Proportion classified', 'Proportion of reads classified'), fontweight='bold')
    medians = []
  
    for s in range(len(settings)):
      setting = settings[s]
      this_setting = []
      for sample in samples:
        #if sample not in samples_plotting: continue
        try:
          val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
        except:
          val = 0
        if metric in alpha_div or metric in beta_div and val == 0:
          val = max(metaphlan.loc[:, metric].values)
        if metric in alpha_div:
          val = val-truth_calcs.loc[sample, metric]
        this_setting.append(val)
      plot = plt.scatter(this_setting, np.random.normal(s+1, scale=0.1, size=len(this_setting)), s=2, alpha=0.1, color=colors[g][s])
      box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5, vert=False)
      for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: change = plt.setp(box[item], color='k')
      medians.append(np.median(this_setting))
      if metric == 'L1 distance':
        plt.text(np.median(this_setting), s+0.7, str(round(np.median(this_setting))), ha='center', va='top')
      else:
        plt.text(np.median(this_setting), s+0.7, str(round(np.median(this_setting), 2)), ha='center', va='top')
    if metric == "Simpson's diversity": plt.plot([0, 0], [0.5, s+1.5], 'k--')
      
    if c == 1: lim = plt.xlim([0, 1.05])
    elif c == 2: lim = plt.xlim([0, 0.8])
    elif c == 3: lim = plt.xlim([5000000, 16000000])
    if g < 3: plt.xticks([])
    else: plt.xlabel(metric)
    
    for a in range(len(axes)):
      plt.sca(axes[a])
      if g == 0: rename_settings['estimated_reads'] = 'Default estimated reads\n(tavg_g/very-sensitive)'
      elif g == 1: rename_settings['estimated_reads'] = 'very-sensitive\n(Default estimated reads)'
      elif g == 2: rename_settings['estimated_reads'] = 'tavg_g: truncated clade global average\n(Default estimated reads)'
      elif g == 3: rename_settings['estimated_reads'] = 'MetaPhlAn 3\nDefault estimated reads'
      if a == 0: 
        plt.yticks([b+1 for b in range(len(settings))], [rename_settings[setting] for setting in settings])
        plt.ylabel(ylabels[g], fontweight='bold')
      else: plt.yticks([b+1 for b in range(len(settings))], [])

  r += ns
  g += 1

rename_settings['estimated_reads'] = 'Default estimated reads\n(tavg_g/very-sensitive)'

if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_all_comparison_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Highest mean F1 score of each

Calculated as the median of all mean F1 scores for each setting in each group of settings.

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = [['default', 'estimated_reads', 'bowtie2_reads'], ['estimated_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local'], ['avg_g', 'avg_l', 'estimated_reads', 'tavg_l', 'wavg_g', 'wavg_l', 'med']]
locations_single = ['upper right', 'lower left', 'upper right', 'lower right']

max_settings = []

for s in range(len(settings)):
  medians = []
  for setting in settings[s]:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(metaphlan.loc[sample+'-MetaPhlAn-'+setting, 'Mean F1 score'])
      except:
        this_setting.append(0)
    medians.append(np.median(this_setting))
  max_value = max([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_settings.append(settings[s][max_index])

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for s in range(len(max_settings)):
    setting = max_settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=10, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_settings[max_settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_settings[max_settings[min_index]]

  anchored_text = AnchoredText(string, loc=locations_single[m])
  ax.add_artist(anchored_text)

  plt.xticks([1, 2, 3], [rename_settings[setting] for setting in max_settings], rotation=90)
  plt.xlim([0.5, 3.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_max_mean_f1_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = [['default', 'estimated_reads', 'bowtie2_reads'], ['estimated_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local'], ['avg_g', 'avg_l', 'estimated_reads', 'tavg_l', 'wavg_g', 'wavg_l', 'med']]

max_settings = []

for s in range(len(settings)):
  medians = []
  for setting in settings[s]:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(metaphlan.loc[sample+'-MetaPhlAn-'+setting, 'Mean F1 score'])
      except:
        this_setting.append(0)
    medians.append(np.median(this_setting))
  max_value = max([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_settings.append(settings[s][max_index])

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for s in range(len(max_settings)):
    setting = max_settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=2, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
  
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_settings[max_settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_settings[max_settings[min_index]]

  anchored_text = AnchoredText(string, loc='upper right')
  ax.add_artist(anchored_text)
  
  if m >= 18:
    plt.xticks([1, 2, 3], [rename_settings[setting] for setting in max_settings], rotation=90)
  else:
    plt.xticks([1, 2, 3], [])
  
  plt.xlim([0.5, 3.5])
  if metric == 'Robust Aitchisons distance':
    plt.ylim([-2, 30])
  elif metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_max_mean_f1_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Lowest L1 distance

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = [['default', 'estimated_reads', 'bowtie2_reads'], ['estimated_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local'], ['avg_g', 'avg_l', 'estimated_reads', 'tavg_l', 'wavg_g', 'wavg_l', 'med']]
locations_single = ['upper right', 'lower left', 'upper right', 'lower right']

max_settings = []

for s in range(len(settings)):
  medians = []
  for setting in settings[s]:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(metaphlan.loc[sample+'-MetaPhlAn-'+setting, 'L1 distance'])
      except:
        this_setting.append(max(metaphlan.loc[:, 'L1 distance'].values))
    medians.append(np.median(this_setting))
  max_value = min([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_settings.append(settings[s][max_index])

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for s in range(len(max_settings)):
    setting = max_settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=10, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_settings[max_settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_settings[max_settings[min_index]]

  anchored_text = AnchoredText(string, loc=locations_single[m])
  ax.add_artist(anchored_text)

  plt.xticks([1, 2, 3], [rename_settings[setting] for setting in max_settings], rotation=90)
  plt.xlim([0.5, 3.5])
  if metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_minimum_l1_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
settings = [['default', 'estimated_reads', 'bowtie2_reads'], ['estimated_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local'], ['avg_g', 'avg_l', 'estimated_reads', 'tavg_l', 'wavg_g', 'wavg_l', 'med']]

max_settings = []

for s in range(len(settings)):
  medians = []
  for setting in settings[s]:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(metaphlan.loc[sample+'-MetaPhlAn-'+setting, 'L1 distance'])
      except:
        this_setting.append(max(metaphlan.loc[:, 'L1 distance'].values))
    medians.append(np.median(this_setting))
  max_value = min([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_settings.append(settings[s][max_index])

for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for s in range(len(max_settings)):
    setting = max_settings[s]
    this_setting = []
    for sample in samples:
      try:
        val = metaphlan.loc[sample+'-MetaPhlAn-'+setting, metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(metaphlan.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_setting.append(val)
    plt.scatter(np.random.normal(s+1, scale=0.1, size=len(this_setting)), this_setting, s=2, alpha=0.1)
    box = plt.boxplot(this_setting, positions=[s+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_setting))
  
  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_settings[max_settings[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_settings[max_settings[min_index]]

  anchored_text = AnchoredText(string, loc='upper right')
  ax.add_artist(anchored_text)
  
  if m >= 18:
    plt.xticks([1, 2, 3], [rename_settings[setting] for setting in max_settings], rotation=90)
  else:
    plt.xticks([1, 2, 3], [])
  
  plt.xlim([0.5, 3.5])
  if metric == 'Robust Aitchisons distance':
    plt.ylim([-2, 30])
  elif metric in limits_metaphlan:
    plt.ylim(limits_metaphlan[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_minimum_l1_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

# 9. Kraken2 vs MetaPhlAn 3 {.tabset}

This is the main comparison with only RefSeq Complete V205, ChocoPhlAn 3 equivalent Kraken2 database and MetaPhlAn 3, taking the version of each of these that is best in each case in terms of mean F1 score or lowest L1 distance.

## Highest mean F1 score {.tabset}

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
refseq_V205 = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', index_col=0, header=0)
chocophlan = pd.read_csv(direc_save+'kraken2_chocophlan_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local', 'avg_g', 'avg_l', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
comp_dbs = [metaphlan, chocophlan, refseq_V205]
db_names = ['MetaPhlAn', 'kraken2_chocophlan', 'kraken2_refseqV205']
limits = [[-0.05, 1.05], [-0.05, 0.85], [-0.05, 0.3], [400000, 20000000]]
location = ['lower left', 'lower right', 'upper right', 'upper right']

max_for_each = {}
for d in range(len(comp_dbs)):
  db = comp_dbs[d]
  medians = []
  if db_names[d] == 'MetaPhlAn':
    different_settings = settings
  else:
    different_settings = confidence
  for setting in different_settings:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(db.loc[sample+'-'+db_names[d]+'-'+setting, 'Mean F1 score'])
      except:
        this_setting.append(0)
    medians.append(np.median(this_setting))
  max_value = max([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_for_each[db_names[d]] = different_settings[max_index]

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for d in range(len(comp_dbs)):
    db = comp_dbs[d]
    this_db = []
    for sample in samples:
      try:
        val = db.loc[sample+'-'+db_names[d]+'-'+max_for_each[db_names[d]], metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(db.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_db.append(val)
    plt.scatter(np.random.normal(d+1, scale=0.1, size=len(this_db)), this_db, s=10, alpha=0.1, color=colors_db[db_names[d]])
    box = plt.boxplot(this_db, positions=[d+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_db))

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_db[db_names[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_db[db_names[min_index]]

  anchored_text = AnchoredText(string, loc=location[m])
  ax.add_artist(anchored_text)
  
  # print(rename_db[db_names[0]], rename_db[db_names[1]], max_for_each[db_names[1]], )
  plt.xticks([1, 2, 3], [rename_db[db_names[0]]+'\nDefault x number of reads', rename_db[db_names[1]]+'\nConfidence threshold = '+max_for_each[db_names[1]], rename_db[db_names[2]]+'\nConfidence threshold = '+max_for_each[db_names[2]]], rotation=90)
  plt.xlim([0.5, 3.5])
  plt.ylim(limits[m])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_vs_kraken_choco_v205_max_mean_f1_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all'}
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
refseq_V205 = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', index_col=0, header=0)
chocophlan = pd.read_csv(direc_save+'kraken2_chocophlan_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local', 'avg_g', 'avg_l', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
comp_dbs = [metaphlan, chocophlan, refseq_V205]
db_names = ['MetaPhlAn', 'kraken2_chocophlan', 'kraken2_refseqV205']

locations["Chao1 richness"] = 'upper left'
locations["Faith's phylogenetic diversity"] = 'upper left'

max_for_each = {}
for d in range(len(comp_dbs)):
  if not saving_figures: break
  db = comp_dbs[d]
  medians = []
  if db_names[d] == 'MetaPhlAn':
    different_settings = settings
  else:
    different_settings = confidence
  for setting in different_settings:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(db.loc[sample+'-'+db_names[d]+'-'+setting, 'Mean F1 score'])
      except:
        this_setting.append(0)
    medians.append(np.median(this_setting))
  max_value = max([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_for_each[db_names[d]] = different_settings[max_index]

for m in range(len(all_metrics)):
  if not saving_figures: break
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for d in range(len(comp_dbs)):
    db = comp_dbs[d]
    this_db = []
    for sample in samples:
      try:
        val = db.loc[sample+'-'+db_names[d]+'-'+max_for_each[db_names[d]], metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(db.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_db.append(val)
    plt.scatter(np.random.normal(d+1, scale=0.1, size=len(this_db)), this_db, s=2, alpha=0.1, color=colors_db[db_names[d]])
    box = plt.boxplot(this_db, positions=[d+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_db))
    if m == 10 or m == 11: print(metric, np.median(this_db))

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_db[db_names[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_db[db_names[min_index]]

  anchored_text = AnchoredText(string, loc=locations[metric])
  ax.add_artist(anchored_text)

  if m >= 18:
    plt.xticks([1, 2, 3], [rename_db[db_names[0]]+'\nDefault x number of reads', rename_db[db_names[1]]+'\nConfidence threshold = '+max_for_each[db_names[1]], rename_db[db_names[2]]+'\nConfidence threshold = '+max_for_each[db_names[2]]], rotation=90)
  else:
    plt.xticks([1, 2, 3], [])

  plt.xlim([0.5, 3.5])
  if metric == 'Robust Aitchisons distance':
    plt.ylim([-2, 30])
  elif metric in limits_all:
    plt.ylim(limits_all[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_vs_kraken_choco_v205_max_mean_f1_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Summary with different options {.tabset}

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
refseq_V205 = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', index_col=0, header=0)
chocophlan = pd.read_csv(direc_save+'kraken2_chocophlan_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local', 'avg_g', 'avg_l', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
comp_dbs = [metaphlan, chocophlan, refseq_V205]
db_names = ['MetaPhlAn', 'kraken2_chocophlan', 'kraken2_refseqV205']
limits = [[-0.05, 1.05], [-0.05, 0.85], [-0.05, 0.3], [400000, 20000000]]

plotting_db = [['estimated_reads', 'sensitive'], ['0.00', '0.35', '0.50', '0.90', '1.00'], ['0.00', '0.15', '0.50', '0.65', '1.00']]
labels = ['Estimated reads: very-sensitive', 'Estimated reads: sensitive', 'Confidence threshold=0.00', 'Confidence threshold=0.35', 'Confidence threshold=0.50', 'Confidence threshold=0.90', 'Confidence threshold=1.00', 'Confidence threshold=0.00', 'Confidence threshold=0.15', 'Confidence threshold=0.50', 'Confidence threshold=0.65', 'Confidence threshold=1.00']
xlabels = ['Proportion', 'Score', 'Difference', 'Distance']

axes = []
for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  axes.append(ax)
  plt.title(metric.replace('Proportion classified', 'Proportion of reads classified'), fontweight='bold')
  medians = []
  loc = 0
  locs = []
  for d in range(len(comp_dbs)):
    db = comp_dbs[d]
    for setting in plotting_db[d]:
      this_db = []
      for sample in samples:
        try:
          val = db.loc[sample+'-'+db_names[d]+'-'+setting, metric]
        except:
          val = 0
        if metric in alpha_div or metric in beta_div:
          if val == 0:
            val = max(db.loc[:, metric].values)
        if metric in alpha_div:
          val = val-truth_calcs.loc[sample, metric]
        if metric in alpha_div: print(sample+'-'+db_names[d]+'-'+setting, val)
        this_db.append(val)
      scat = plt.scatter(this_db, np.random.normal(loc+1, scale=0.1, size=len(this_db)), s=10, alpha=0.1, color=colors_db[db_names[d]])
      box = plt.boxplot(this_db, positions=[loc+1], showfliers=False, widths=0.5, vert=False)
      for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: lines = plt.setp(box[item], color='k')
      med = np.median(this_db)
      medians.append(med)
      if metric == 'L1 distance': plt.text(med, loc+0.7, str(round(med)), ha='center', va='top')
      elif metric == "Simpson's diversity": plt.text(med, loc+0.7, str(round(med, 3)), ha='center', va='top')
      else: plt.text(med, loc+0.7, str(round(med, 2)), ha='center', va='top')
      if metric == "Simpson's diversity": simps = plt.plot([0, 0], [0, loc+1.5], 'k--')
      locs.append(loc+1)
      loc += 1
    loc += 1
  
  if metric == 'Proportion classified':
    plt.yticks(locs, labels)
  else: plt.yticks(locs, [])
  plt.xlabel(xlabels[m])
    
  
  # print(rename_db[db_names[0]], rename_db[db_names[1]], max_for_each[db_names[1]], )
  #plt.xticks([1, 2, 3], [rename_db[db_names[0]]+'\nDefault x number of reads', rename_db[db_names[1]]+'\nConfidence threshold = '+max_for_each[db_names[1]], rename_db[db_names[2]]+'\nConfidence threshold = '+max_for_each[db_names[2]]], rotation=90)
  #plt.xlim([0.5, 3.5])
  if metric in alpha_div: plt.xlim([-0.10, 0.05])
  else: plt.xlim(limits[m])
  plt.ylim([0.25, loc-0.5])

plt.sca(axes[0])
plt.text(-0.7, 1.5, 'MetaPhlAn 3', ha='center', va='center', rotation=90, fontweight='bold')
plt.text(-0.7, 6, 'Kraken2\nChocoPhlAn 3', ha='center', va='center', rotation=90, fontweight='bold')
plt.text(-0.7, 12, 'Kraken2\nNCBI RefSeq Complete V205', ha='center', va='center', rotation=90, fontweight='bold')

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_vs_kraken_choco_v205_horizontal_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
limits_all = {"Simpson's diversity":[-0.1, 0.1], "Shannon diversity":[-1, 2], "Faith's phylogenetic diversity":[-1000, 15000], "Chao1 richness":[-1000, 11000], "McIntosh's evenness":[-0.1, 0.2], "Pielou evenness":[-0.3, 0.2], "Simpson's evenness": [-0.25, 0.25], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-1, 40], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

locations_new = {"Simpson's diversity":'upper left', "Shannon diversity":'lower left', "Faith's phylogenetic diversity":'upper right', "Chao1 richness":'upper right', "McIntosh's evenness":'lower left', "Pielou evenness":'lower right', "Simpson's evenness":'lower right', "L1 distance":'lower right', "Robust Aitchisons distance":'upper left', "Bray-Curtis dissimilarity relative abundance":'upper left', "Weighted unifrac distance relative abundance":'upper left', "Unweighted unifrac distance relative abundance":'upper left', "Aitchisons distance":'upper right', "Bray-Curtis dissimilarity raw":'upper left', "Weighted unifrac distance raw":'upper left', "Unweighted unifrac distance raw":'upper right', "Proportion classified":'lower left', "Precision taxa":'upper left', "Recall taxa":'lower left', "F1 score taxa":'lower right', "Precision reads":'upper left', "Recall reads":'lower left', "F1 score reads":'lower right', "Mean F1 score":'upper left'}
  
locations_new["Chao1 richness"] = 'upper left'
locations_new["Faith's phylogenetic diversity"] = 'upper left'
  
plt.figure(figsize=(20,24))
all_metrics = prec_rec_f1+alpha_div+beta_div
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
refseq_V205 = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', index_col=0, header=0)
chocophlan = pd.read_csv(direc_save+'kraken2_chocophlan_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local', 'avg_g', 'avg_l', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
comp_dbs = [metaphlan, chocophlan, refseq_V205]
db_names = ['MetaPhlAn', 'kraken2_chocophlan', 'kraken2_refseqV205']
limits = [[-0.05, 1.05], [-0.05, 0.85], [-0.05, 0.3], [400000, 20000000]]
location_single = ['lower left', 'lower right', 'upper right', 'upper right']

plotting_db = [['estimated_reads', 'sensitive'], ['0.00', '0.35', '0.50', '0.90', '1.00'], ['0.00', '0.15', '0.50', '0.60', '1.00']]
labels = ['Estimated reads: very-sensitive', 'Estimated reads: sensitive', 'Confidence threshold=0.00', 'Confidence threshold=0.35', 'Confidence threshold=0.50', 'Confidence threshold=0.90', 'Confidence threshold=1.00', 'Confidence threshold=0.00', 'Confidence threshold=0.15', 'Confidence threshold=0.50', 'Confidence threshold=0.60', 'Confidence threshold=1.00']
xlabels = ['Proportion', 'Score', 'Difference', 'Distance']

axes = []
for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  axes.append(ax)
  plt.title(metric.replace('Proportion classified', 'Proportion of reads classified').replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  loc = 0
  locs = []
  for d in range(len(comp_dbs)):
    db = comp_dbs[d]
    for setting in plotting_db[d]:
      this_db = []
      for sample in samples:
        try:
          val = db.loc[sample+'-'+db_names[d]+'-'+setting, metric]
        except:
          val = 0
        if metric in alpha_div or metric in beta_div:
          if val == 0:
            val = max(db.loc[:, metric].values)
        if metric in alpha_div:
          val = val-truth_calcs.loc[sample, metric]
        if metric in alpha_div: print(sample+'-'+db_names[d]+'-'+setting, val)
        this_db.append(val)
      scat = plt.scatter(this_db, np.random.normal(loc+1, scale=0.1, size=len(this_db)), s=10, alpha=0.1, color=colors_db[db_names[d]])
      box = plt.boxplot(this_db, positions=[loc+1], showfliers=False, widths=0.5, vert=False)
      for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: lines = plt.setp(box[item], color='k')
      med = np.median(this_db)
      medians.append(med)
      if metric == 'L1 distance': plt.text(med, loc+0.7, str(round(med)), ha='center', va='top')
      elif metric == "Simpson's diversity": plt.text(med, loc+0.7, str(round(med, 3)), ha='center', va='top')
      else: plt.text(med, loc+0.7, str(round(med, 2)), ha='center', va='top')
      if metric == "Simpson's diversity": simps = plt.plot([0, 0], [0, loc+1.5], 'k--')
      locs.append(loc+1)
      loc += 1
    loc += 1

  if m % 6 == 0:
    plt.yticks(locs, labels)
    plt.text(-1.15, 0.09, 'MetaPhlAn 3', ha='center', va='center', rotation=90, fontweight='bold', transform=ax.transAxes)
    plt.text(-1.15, 0.39, 'Kraken2\nChocoPhlAn 3', ha='center', va='center', rotation=90, fontweight='bold', transform=ax.transAxes)
    plt.text(-1.15, 0.82, 'Kraken2\nNCBI RefSeq\nComplete V205', ha='center', va='center', rotation=90, fontweight='bold', transform=ax.transAxes)
  else: plt.yticks(locs, [])
  if metric in limits_all:
    plt.xlim(limits_all[metric])
  else: plt.xlim([-0.05, 1.10])
  plt.ylim([0.25, loc-0.5])

#plt.tight_layout()
#plt.show()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_vs_kraken_choco_v205_horizontal_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

## Lowest L1 distance {.tabset}

### Reduced metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(20,8))
limited_metrics = ['Proportion classified', 'Mean F1 score', "Simpson's diversity", 'L1 distance']
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
refseq_V205 = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', index_col=0, header=0)
chocophlan = pd.read_csv(direc_save+'kraken2_chocophlan_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local', 'avg_g', 'avg_l', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
comp_dbs = [metaphlan, chocophlan, refseq_V205]
db_names = ['MetaPhlAn', 'kraken2_chocophlan', 'kraken2_refseqV205']
limits = [[-0.05, 1.05], [-0.05, 0.85], [-0.05, 0.3], [400000, 20000000]]
location = ['lower right', 'lower right', 'upper right', 'upper right']

max_for_each = {}
for d in range(len(comp_dbs)):
  db = comp_dbs[d]
  medians = []
  if db_names[d] == 'MetaPhlAn':
    different_settings = settings
  else:
    different_settings = confidence
  for setting in different_settings:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(db.loc[sample+'-'+db_names[d]+'-'+setting, 'L1 distance'])
      except:
        this_setting.append(max(db.loc[:, 'L1 distance'].values))
    medians.append(np.median(this_setting))
  max_value = min([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_for_each[db_names[d]] = different_settings[max_index]

for m in range(len(limited_metrics)):
  metric = limited_metrics[m]
  ax = plt.subplot(1,4,m+1)
  plt.sca(ax)
  plt.title(metric, fontweight='bold')
  medians = []
  for d in range(len(comp_dbs)):
    db = comp_dbs[d]
    this_db = []
    for sample in samples:
      try:
        val = db.loc[sample+'-'+db_names[d]+'-'+max_for_each[db_names[d]], metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(db.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_db.append(val)
    plt.scatter(np.random.normal(d+1, scale=0.1, size=len(this_db)), this_db, s=10, alpha=0.1, color=colors_db[db_names[d]])
    box = plt.boxplot(this_db, positions=[d+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_db))

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_db[db_names[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_db[db_names[min_index]]

  anchored_text = AnchoredText(string, loc=location[m])
  ax.add_artist(anchored_text)

  # print(rename_db[db_names[0]], rename_db[db_names[1]], max_for_each[db_names[1]], )
  plt.xticks([1, 2, 3], [rename_db[db_names[0]]+'\nTruncated average of length-\nnormalized marker counts', rename_db[db_names[1]]+'\nConfidence threshold = '+max_for_each[db_names[1]], rename_db[db_names[2]]+'\nConfidence threshold = '+max_for_each[db_names[2]]], rotation=90)
  plt.xlim([0.5, 3.5])
  plt.ylim(limits[m])

plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_vs_kraken_choco_v205_min_l1_reduced_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

### All metrics

```{python, results='hide', fig.keep='all' }
plt.figure(figsize=(30,20))
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
refseq_V205 = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', index_col=0, header=0)
chocophlan = pd.read_csv(direc_save+'kraken2_chocophlan_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local', 'avg_g', 'avg_l', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
comp_dbs = [metaphlan, chocophlan, refseq_V205]
db_names = ['MetaPhlAn', 'kraken2_chocophlan', 'kraken2_refseqV205']
  
limits_all = {"Proportion classified":[-0.01, 1.01], "Simpson's diversity":[-0.05, 0.3], "Shannon diversity":[0, 10], "Faith's phylogenetic diversity":[0, 55000], "Chao1 richness":[0, 45000], "McIntosh's evenness":[0.2, 1], "Pielou evenness":[-0.1, 0.6], "Simpson's evenness": [0.6, 1.05], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 100], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

locations = {"Simpson's diversity":'upper left', "Shannon diversity":'lower left', "Faith's phylogenetic diversity":'upper right', "Chao1 richness":'upper right', "McIntosh's evenness":'lower left', "Pielou evenness":'lower right', "Simpson's evenness":'lower right', "L1 distance":'lower right', "Robust Aitchisons distance":'upper left', "Bray-Curtis dissimilarity relative abundance":'upper left', "Weighted unifrac distance relative abundance":'upper left', "Unweighted unifrac distance relative abundance":'upper left', "Aitchisons distance":'upper right', "Bray-Curtis dissimilarity raw":'upper left', "Weighted unifrac distance raw":'upper left', "Unweighted unifrac distance raw":'upper right', "Proportion classified":'lower left', "Precision taxa":'upper left', "Recall taxa":'lower left', "F1 score taxa":'lower right', "Precision reads":'upper left', "Recall reads":'lower left', "F1 score reads":'lower right', "Mean F1 score":'upper left'}
  
locations["Chao1 richness"] = 'upper left'
locations["Faith's phylogenetic diversity"] = 'upper left'
  
max_for_each = {}
for d in range(len(comp_dbs)):
  db = comp_dbs[d]
  medians = []
  if db_names[d] == 'MetaPhlAn':
    different_settings = settings
  else:
    different_settings = confidence
  for setting in different_settings:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(db.loc[sample+'-'+db_names[d]+'-'+setting, 'L1 distance'])
      except:
        this_setting.append(max(db.loc[:, 'L1 distance'].values))
    medians.append(np.median(this_setting))
  max_value = min([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_for_each[db_names[d]] = different_settings[max_index]
  
for m in range(len(all_metrics)):
  metric = all_metrics[m]
  ax = plt.subplot(4,6,m+1)
  plt.sca(ax)
  plt.title(metric.replace('relative abundance', '\nrelative abundance').replace('raw', '\nraw'), fontweight='bold')
  medians = []
  for d in range(len(comp_dbs)):
    db = comp_dbs[d]
    this_db = []
    for sample in samples:
      try:
        val = db.loc[sample+'-'+db_names[d]+'-'+max_for_each[db_names[d]], metric]
      except:
        val = 0
      if metric in alpha_div or metric in beta_div and val == 0:
        val = max(db.loc[:, metric].values)
      if metric in alpha_div:
        val = val-truth_calcs.loc[sample, metric]
      this_db.append(val)
    plt.scatter(np.random.normal(d+1, scale=0.1, size=len(this_db)), this_db, s=2, alpha=0.1, color=colors_db[db_names[d]])
    box = plt.boxplot(this_db, positions=[d+1], showfliers=False, widths=0.5)
    for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
    medians.append(np.median(this_db))
    if m == 10 or m == 11: print(metric, np.median(this_db))

  if metric in prec_rec_f1:
    max_value = max([abs(val) for val in medians])
    max_index = [abs(val) for val in medians].index(max_value)
    max_value = medians[max_index]
    string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_db[db_names[max_index]]
  else:
    min_value = min([abs(val) for val in medians])
    min_index = [abs(val) for val in medians].index(min_value)
    min_value = medians[min_index]
    string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_db[db_names[min_index]]

  anchored_text = AnchoredText(string, loc=locations[metric])
  ax.add_artist(anchored_text)
  
  if m >= 18:
    plt.xticks([1, 2, 3], [rename_db[db_names[0]]+'\nDefault x number of reads', rename_db[db_names[1]]+'\nConfidence threshold = '+max_for_each[db_names[1]], rename_db[db_names[2]]+'\nConfidence threshold = '+max_for_each[db_names[2]]], rotation=90)
  else:
    plt.xticks([1, 2, 3], [])
    
  plt.xlim([0.5, 3.5])
  if metric == 'Robust Aitchisons distance':
    plt.ylim([-2, 30])
  elif metric in limits_all:
    plt.ylim(limits_all[metric])

#plt.tight_layout()
if saving_figures:
  plt.savefig(direc_save+'figures/metaphlan_vs_kraken_choco_v205_min_l1_all_metrics.png', dpi=600, bbox_inches='tight')
  plt.show()
else:
  plt.show()
```

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/metaphlan_vs_kraken_choco_v205_min_l1_all_metrics.png)

## Effect of sample characteristics {.tabset}

Now I'll only use the maximum mean F1 score settings.

```{python, results='hide', fig.keep='all' }
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
all_metrics = prec_rec_f1+alpha_div+beta_div
truth_calcs = pd.read_csv(direc_save+'truth_calculations.csv', index_col=0, header=0)
metaphlan = pd.read_csv(direc_save+'MetaPhlAn_calculations.csv', index_col=0, header=0)
refseq_V205 = pd.read_csv(direc_save+'kraken2_refseqV205_calculations.csv', index_col=0, header=0)
chocophlan = pd.read_csv(direc_save+'kraken2_chocophlan_calculations.csv', index_col=0, header=0)
settings = ['default', 'estimated_reads', 'bowtie2_reads', 'very_sensitive_local', 'sensitive', 'sensitive_local', 'avg_g', 'avg_l', 'tavg_l', 'wavg_g', 'wavg_l', 'med']
comp_dbs = [metaphlan, chocophlan, refseq_V205]
db_names = ['MetaPhlAn', 'kraken2_chocophlan', 'kraken2_refseqV205']

limits_all = {"Proportion classified":[-0.01, 1.01], "Simpson's diversity":[0, 0.5], "Shannon diversity":[0, 10], "Faith's phylogenetic diversity":[0, 55000], "Chao1 richness":[0, 45000], "McIntosh's evenness":[0.2, 1], "Pielou evenness":[-0.1, 0.6], "Simpson's evenness": [0.6, 1.05], "L1 distance":[-100, 20000000], "Robust Aitchisons distance":[-2, 100], "Bray-Curtis dissimilarity relative abundance":[-0.05, 1.05], "Weighted unifrac distance relative abundance":[-10, 300], "Unweighted unifrac distance relative abundance":[-0.05, 1.05], "Aitchisons distance":[-10, 250], "Bray-Curtis dissimilarity raw":[-0.05, 1.05], "Weighted unifrac distance raw":[-10, 175], "Unweighted unifrac distance raw":[-0.05, 1.05]}

locations = {"Simpson's diversity":'upper left', "Shannon diversity":'lower left', "Faith's phylogenetic diversity":'upper right', "Chao1 richness":'upper right', "McIntosh's evenness":'lower left', "Pielou evenness":'lower right', "Simpson's evenness":'lower right', "L1 distance":'lower right', "Robust Aitchisons distance":'upper left', "Bray-Curtis dissimilarity relative abundance":'upper left', "Weighted unifrac distance relative abundance":'upper left', "Unweighted unifrac distance relative abundance":'upper left', "Aitchisons distance":'upper right', "Bray-Curtis dissimilarity raw":'upper left', "Weighted unifrac distance raw":'upper left', "Unweighted unifrac distance raw":'upper right', "Proportion classified":'lower left', "Precision taxa":'upper left', "Recall taxa":'lower left', "F1 score taxa":'lower right', "Precision reads":'upper left', "Recall reads":'lower left', "F1 score reads":'lower right', "Mean F1 score":'upper left'}

ani = ['ani100', 'ani99', 'ani97', 'ani95']
species_diversity = ['cLOW', 'cHIGH']
strain_diversity = ['stFalse', 'stTrue']

max_for_each = {}
for d in range(len(comp_dbs)):
  db = comp_dbs[d]
  medians = []
  if db_names[d] == 'MetaPhlAn':
    different_settings = settings
  else:
    different_settings = confidence
  for setting in different_settings:
    this_setting = []
    for sample in samples:
      try:
        this_setting.append(db.loc[sample+'-'+db_names[d]+'-'+setting, 'L1 distance'])
      except:
        this_setting.append(max(db.loc[:, 'L1 distance'].values))
    medians.append(np.median(this_setting))
  max_value = min([abs(val) for val in medians])
  max_index = [abs(val) for val in medians].index(max_value)
  max_value = medians[max_index]
  max_for_each[db_names[d]] = different_settings[max_index]

if saving_figures:
  for m in range(len(all_metrics)):
    plt.figure(figsize=(15,15))
    metric = all_metrics[m]
    if metric not in alpha_div: continue
    count = 0
    for an in ani:
      for spec in species_diversity:
          for strn in strain_diversity:
            this_samples = []
            for sample in samples:
              if an in sample and spec in sample and strn in sample: this_samples.append(sample)
  
            if len(this_samples) == 0: continue
            ax = plt.subplot(4,4,count+1)
            plt.sca(ax)
            ax.set_title(an.replace('ani', 'ANI')+'\nSpecies diversity = '+spec.replace('c', '')+'\nStrain diversity = '+strn.replace('st', ''))
            medians = []
            for d in range(len(comp_dbs)):
              db = comp_dbs[d]
              this_db = []
              for sample in this_samples:
                try: val = db.loc[sample+'-'+db_names[d]+'-'+max_for_each[db_names[d]], metric]
                except: val = 0
                if metric in alpha_div or metric in beta_div and val == 0: val = max(db.loc[:, metric].values)
                if metric in alpha_div: val = val-truth_calcs.loc[sample, metric]
                this_db.append(val)
              plt.scatter(np.random.normal(d+1, scale=0.1, size=len(this_db)), this_db, s=10, alpha=0.2, color=colors_db[db_names[d]])
              box = plt.boxplot(this_db, positions=[d+1], showfliers=False, widths=0.5)
              for item in ['boxes', 'whiskers', 'fliers', 'medians', 'caps']: plt.setp(box[item], color='k')
              medians.append(np.median(this_db))
            if metric in prec_rec_f1:
              max_value = max([abs(val) for val in medians])
              max_index = [abs(val) for val in medians].index(max_value)
              max_value = medians[max_index]
              string = 'Maximum = '+str(round(max_value, 3))+' for\n'+rename_db[db_names[max_index]]
            else:
              min_value = min([abs(val) for val in medians])
              min_index = [abs(val) for val in medians].index(min_value)
              min_value = medians[min_index]
              string = 'Minimum = '+str(round(min_value, 3))+' for\n'+rename_db[db_names[min_index]]
  
            anchored_text = AnchoredText(string, loc=locations[metric])
            ax.add_artist(anchored_text)
  
            if count > 9:
              plt.xticks([1, 2, 3], [rename_db[db_names[0]]+'\nDefault x number of reads', rename_db[db_names[1]]+'\nConfidence threshold = '+max_for_each[db_names[1]], rename_db[db_names[2]]+'\nConfidence threshold = '+max_for_each[db_names[2]]], rotation=90)
            else:
              plt.xticks([1, 2, 3], [])
  
            if metric == 'Robust Aitchisons distance':
              plt.ylim([-2, 30])
            elif metric in limits_all:
              plt.ylim(limits_all[metric])
            #plt.tight_layout()
            count += 1
    #plt.tight_layout()
    plt.subplots_adjust(hspace=0.4)
    plt.savefig(direc_save+'figures/sample_characteristics_metaphlan_vs_kraken/'+metric.replace(' ', '_')+'.png', dpi=600, bbox_inches='tight')
    plt.close()
``` 

### Proportion classified

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Proportion_classified.png)

### Precision taxa

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Precision_taxa.png)

### Recall taxa

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Recall_taxa.png)

### F1 score taxa

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/F1_score_taxa.png)

### Precision reads

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Precision_reads.png)

### Recall reads

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Recall_reads.png)

### F1 score reads

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/F1_score_reads.png)

### Mean F1 score

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Mean_F1_score.png)

### Simpson's diversity

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Simpson's_diversity.png)

### Shannon diversity

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Shannon_diversity.png)

### Faith's phylogenetic diversity

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Faith's_phylogenetic_diversity.png)

### Chao1 richness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Chao1_richness.png)

### McIntosh's evenness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/McIntosh's_evenness.png)

### Pielou evenness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Pielou_evenness.png)

### Simpson's evenness

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Simpson's_evenness.png)

### L1 distance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/L1_distance.png)

### Robust Aitchisons distance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Robust_Aitchisons_distance.png)

### Bray-Curtis dissimilarity relative abundance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Bray-Curtis_dissimilarity_relative_abundance.png)

### Weighted unifrac distance relative abundance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Weighted_unifrac_distance_relative_abundance.png)

### Unweighted unifrac distance relative abundance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Unweighted_unifrac_distance_relative_abundance.png)

### Aitchisons distance

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Aitchisons_distance.png)

### Bray-Curtis dissimilarity raw

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Bray-Curtis_dissimilarity_raw.png)

### Weighted unifrac distance raw

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Weighted_unifrac_distance_raw.png)

### Unweighted unifrac distance raw

![](/Users/robynwright/Documents/OneDrive/Langille_Lab_postdoc/kraken_confidence_testing_mock/analysis/figures/sample_characteristics_metaphlan_vs_kraken/Unweighted_unifrac_distance_raw.png)
# 10. Normalising for genome size in mock community samples



# 11. Looking at proportion classified and alpha/beta diversity in samples with paired 16S/metagenome data
Samples Gavin used for PICRUSt2?

# 12. Times to run

```{python, results='hide', fig.keep='all'}
times = os.listdir(direc+'times/')
for time in times:
  if time == '.DS_Store': continue
  if 'averages' in time: continue
  this_time = pd.read_csv(direc+'times/'+time, index_col=0, header=0)
  if time != 'metaphlan_time.csv':
    continue
    confidence_str = []
    for row in this_time.index.values:
      confidence_str.append(row.rsplit('_', 1)[1])
    this_time['Confidence'] = confidence_str
    averages = []
    for conf in confidence:
      this_confidence = pd.DataFrame(this_time[this_time['Confidence'] == conf]).fillna(value=0)
      this_confidence[['CPU percent', '%']] = this_confidence['CPU (%)'].str.split('%', expand=True)
      this_confidence = this_confidence.drop(['CPU (%)', '%'], axis=1)
      this_confidence['CPU percent'] = pd.to_numeric(this_confidence['CPU percent'])
      for row in this_confidence.index.values:
        time_str = str(this_confidence.loc[row, 'Wall time (h:mm:ss or m:ss)'])
        if time_str.count(':') > 1: 
          hours, mins, secs = time_str.split(':')[0], time_str.split(':')[1], time_str.split(':')[2]
          mins = float(mins)/60
          secs = float(secs)/60/60
          hours = float(hours)+mins+secs
        elif time_str.count(':') == 1:
          mins, secs = time_str.split(':')[0], time_str.split(':')[1]
          mins = float(mins)/60
          secs = float(secs)/60/60
          hours = mins+secs
        else:
          hours = float(time_str)/60/60
        this_confidence.loc[row, 'Wall time (h:mm:ss or m:ss)'] = hours
      this_average = [time.replace('.csv', '')]
      for col in this_confidence.columns:
        if col != 'Confidence':
          try:
            mean = np.mean(list(this_confidence.loc[:, col].values))
            this_average.append(mean)
          except:
            this_average.append(col)
      this_average.append(conf)
      averages.append(this_average)
    all_averages = pd.DataFrame(averages, columns=['File name']+list(this_time.columns)).set_index('File name').fillna(value=0)
    all_averages['Maximum set size (kb)'] = all_averages['Maximum set size (kb)']/1000000
    all_averages['User time (s)'] = all_averages['User time (s)']/60
    all_averages['System time (s)'] = all_averages['System time (s)']/60
    all_averages['Wall time (h:mm:ss or m:ss)'] = all_averages['Wall time (h:mm:ss or m:ss)']*60
    all_averages = all_averages.rename(columns={'Maximum set size (kb)':'Maximum set size (Gb)', 'User time (s)':'User time (m)', 'System time (s)':'System time (m)', 'Wall time (h:mm:ss or m:ss)':'Wall time (m)'})
    all_averages.to_csv(direc+'times/'+time.replace('.csv', '_averages.csv'))
  else:
    samples = this_time.index.values
    rename = {}
    remove = []
    for sample in samples:
      rename[sample] = sample.replace('metaphlan_reads', 'metaphlan_reads_').replace('.fastq', '').replace('.fasta', '')
      if 'bowtie2out' in sample: remove.append(sample)
    this_time = this_time.drop(remove, axis=0).rename(index=rename)
    samples = list(this_time.index.values)
    settings = ['wavg_g', 'wavg_l', 'tavg_l', 'avg_g', 'avg_l', 'med', 'very-sensitive-local', 'sensitive-local', 'sensitive', 'metaphlan_reads', 'none']
    sample_groups = [[] for g in range(len(settings))]
    for sample in samples:
      got_it = False
      for s in range(len(settings)):
        if settings[s] in sample: 
          sample_groups[s].append(sample)
          got_it = True
          break
      if not got_it:
        sample_groups[-1].append(sample)
    
    averages = []
    for g in range(len(sample_groups)):
      group = sample_groups[g]
      this_group = pd.DataFrame(this_time.loc[group, :]).fillna(value=0)
      this_group[['CPU percent', '%']] = this_group['CPU (%)'].str.split('%', expand=True)
      this_group = this_group.drop(['CPU (%)', '%'], axis=1)
      this_group['CPU percent'] = pd.to_numeric(this_group['CPU percent'])
      for row in this_group.index.values:
        time_str = str(this_group.loc[row, 'Wall time (h:mm:ss or m:ss)'])
        if time_str.count(':') > 1:
          hours, mins, secs = time_str.split(':')[0], time_str.split(':')[1], time_str.split(':')[2]
          mins = float(mins)/60
          secs = float(secs)/60/60
          hours = float(hours)+mins+secs
        elif time_str.count(':') == 1:
          mins, secs = time_str.split(':')[0], time_str.split(':')[1]
          mins = float(mins)/60
          secs = float(secs)/60/60
          hours = mins+secs
        else:
          hours = float(time_str)/60/60
        this_group.loc[row, 'Wall time (h:mm:ss or m:ss)'] = hours
      this_average = [time.replace('.csv', '')]
      for col in this_group.columns:
        if col != 'Confidence':
          try:
            mean = np.mean(list(this_group.loc[:, col].values))
            this_average.append(mean)
          except:
            this_average.append(col)
      this_average.append(settings[g])
      averages.append(this_average)
    all_averages = pd.DataFrame(averages, columns=['File name']+list(this_time.columns)+['Setting']).set_index('File name').fillna(value=0)
    all_averages['Maximum set size (kb)'] = all_averages['Maximum set size (kb)']/1000000
    all_averages['User time (s)'] = all_averages['User time (s)']/60
    all_averages['System time (s)'] = all_averages['System time (s)']/60
    all_averages['Wall time (h:mm:ss or m:ss)'] = all_averages['Wall time (h:mm:ss or m:ss)']*60
    all_averages = all_averages.rename(columns={'Maximum set size (kb)':'Maximum set size (Gb)', 'User time (s)':'User time (m)', 'System time (s)':'System time (m)', 'Wall time (h:mm:ss or m:ss)':'Wall time (m)'})
    all_averages.to_csv(direc+'times/'+time.replace('.csv', '_averages.csv'))
```